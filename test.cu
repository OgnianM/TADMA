#include "include/tadma/Tadma.hpp"
#include <iostream>

using namespace tadma;
using ALLOCATOR = Allocator<kCUDA>;

struct Model {
    std::ifstream bin;

    Tensor<float, ALLOCATOR, Sequence<30522,768>> embeddings_word_embeddings_weight = Tensor<float, ALLOCATOR, Sequence<30522,768>>(bin, 0ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> embeddings_LayerNorm_weight = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 93763584ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> embeddings_LayerNorm_bias = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 93766656ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_0_attention_self_query_bias = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 93769728ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_0_attention_self_key_bias = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 93772800ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_0_attention_self_value_bias = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 93775872ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_0_attention_output_dense_bias = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 93778944ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_0_attention_output_LayerNorm_weight = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 93782016ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_0_attention_output_LayerNorm_bias = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 93785088ULL);
    Tensor<float, ALLOCATOR, Sequence<3072>> encoder_layer_0_intermediate_dense_bias = Tensor<float, ALLOCATOR, Sequence<3072>>(bin, 93788160ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_0_output_dense_bias = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 93800448ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_0_output_LayerNorm_weight = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 93803520ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_0_output_LayerNorm_bias = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 93806592ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_1_attention_self_query_bias = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 93809664ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_1_attention_self_key_bias = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 93812736ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_1_attention_self_value_bias = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 93815808ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_1_attention_output_dense_bias = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 93818880ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_1_attention_output_LayerNorm_weight = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 93821952ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_1_attention_output_LayerNorm_bias = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 93825024ULL);
    Tensor<float, ALLOCATOR, Sequence<3072>> encoder_layer_1_intermediate_dense_bias = Tensor<float, ALLOCATOR, Sequence<3072>>(bin, 93828096ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_1_output_dense_bias = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 93840384ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_1_output_LayerNorm_weight = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 93843456ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_1_output_LayerNorm_bias = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 93846528ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_2_attention_self_query_bias = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 93849600ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_2_attention_self_key_bias = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 93852672ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_2_attention_self_value_bias = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 93855744ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_2_attention_output_dense_bias = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 93858816ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_2_attention_output_LayerNorm_weight = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 93861888ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_2_attention_output_LayerNorm_bias = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 93864960ULL);
    Tensor<float, ALLOCATOR, Sequence<3072>> encoder_layer_2_intermediate_dense_bias = Tensor<float, ALLOCATOR, Sequence<3072>>(bin, 93868032ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_2_output_dense_bias = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 93880320ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_2_output_LayerNorm_weight = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 93883392ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_2_output_LayerNorm_bias = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 93886464ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_3_attention_self_query_bias = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 93889536ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_3_attention_self_key_bias = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 93892608ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_3_attention_self_value_bias = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 93895680ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_3_attention_output_dense_bias = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 93898752ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_3_attention_output_LayerNorm_weight = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 93901824ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_3_attention_output_LayerNorm_bias = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 93904896ULL);
    Tensor<float, ALLOCATOR, Sequence<3072>> encoder_layer_3_intermediate_dense_bias = Tensor<float, ALLOCATOR, Sequence<3072>>(bin, 93907968ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_3_output_dense_bias = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 93920256ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_3_output_LayerNorm_weight = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 93923328ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_3_output_LayerNorm_bias = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 93926400ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_4_attention_self_query_bias = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 93929472ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_4_attention_self_key_bias = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 93932544ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_4_attention_self_value_bias = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 93935616ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_4_attention_output_dense_bias = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 93938688ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_4_attention_output_LayerNorm_weight = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 93941760ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_4_attention_output_LayerNorm_bias = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 93944832ULL);
    Tensor<float, ALLOCATOR, Sequence<3072>> encoder_layer_4_intermediate_dense_bias = Tensor<float, ALLOCATOR, Sequence<3072>>(bin, 93947904ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_4_output_dense_bias = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 93960192ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_4_output_LayerNorm_weight = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 93963264ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_4_output_LayerNorm_bias = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 93966336ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_5_attention_self_query_bias = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 93969408ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_5_attention_self_key_bias = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 93972480ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_5_attention_self_value_bias = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 93975552ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_5_attention_output_dense_bias = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 93978624ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_5_attention_output_LayerNorm_weight = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 93981696ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_5_attention_output_LayerNorm_bias = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 93984768ULL);
    Tensor<float, ALLOCATOR, Sequence<3072>> encoder_layer_5_intermediate_dense_bias = Tensor<float, ALLOCATOR, Sequence<3072>>(bin, 93987840ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_5_output_dense_bias = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 94000128ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_5_output_LayerNorm_weight = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 94003200ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_5_output_LayerNorm_bias = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 94006272ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_6_attention_self_query_bias = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 94009344ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_6_attention_self_key_bias = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 94012416ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_6_attention_self_value_bias = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 94015488ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_6_attention_output_dense_bias = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 94018560ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_6_attention_output_LayerNorm_weight = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 94021632ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_6_attention_output_LayerNorm_bias = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 94024704ULL);
    Tensor<float, ALLOCATOR, Sequence<3072>> encoder_layer_6_intermediate_dense_bias = Tensor<float, ALLOCATOR, Sequence<3072>>(bin, 94027776ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_6_output_dense_bias = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 94040064ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_6_output_LayerNorm_weight = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 94043136ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_6_output_LayerNorm_bias = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 94046208ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_7_attention_self_query_bias = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 94049280ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_7_attention_self_key_bias = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 94052352ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_7_attention_self_value_bias = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 94055424ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_7_attention_output_dense_bias = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 94058496ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_7_attention_output_LayerNorm_weight = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 94061568ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_7_attention_output_LayerNorm_bias = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 94064640ULL);
    Tensor<float, ALLOCATOR, Sequence<3072>> encoder_layer_7_intermediate_dense_bias = Tensor<float, ALLOCATOR, Sequence<3072>>(bin, 94067712ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_7_output_dense_bias = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 94080000ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_7_output_LayerNorm_weight = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 94083072ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_7_output_LayerNorm_bias = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 94086144ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_8_attention_self_query_bias = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 94089216ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_8_attention_self_key_bias = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 94092288ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_8_attention_self_value_bias = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 94095360ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_8_attention_output_dense_bias = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 94098432ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_8_attention_output_LayerNorm_weight = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 94101504ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_8_attention_output_LayerNorm_bias = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 94104576ULL);
    Tensor<float, ALLOCATOR, Sequence<3072>> encoder_layer_8_intermediate_dense_bias = Tensor<float, ALLOCATOR, Sequence<3072>>(bin, 94107648ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_8_output_dense_bias = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 94119936ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_8_output_LayerNorm_weight = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 94123008ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_8_output_LayerNorm_bias = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 94126080ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_9_attention_self_query_bias = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 94129152ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_9_attention_self_key_bias = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 94132224ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_9_attention_self_value_bias = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 94135296ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_9_attention_output_dense_bias = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 94138368ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_9_attention_output_LayerNorm_weight = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 94141440ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_9_attention_output_LayerNorm_bias = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 94144512ULL);
    Tensor<float, ALLOCATOR, Sequence<3072>> encoder_layer_9_intermediate_dense_bias = Tensor<float, ALLOCATOR, Sequence<3072>>(bin, 94147584ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_9_output_dense_bias = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 94159872ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_9_output_LayerNorm_weight = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 94162944ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_9_output_LayerNorm_bias = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 94166016ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_10_attention_self_query_bias = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 94169088ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_10_attention_self_key_bias = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 94172160ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_10_attention_self_value_bias = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 94175232ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_10_attention_output_dense_bias = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 94178304ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_10_attention_output_LayerNorm_weight = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 94181376ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_10_attention_output_LayerNorm_bias = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 94184448ULL);
    Tensor<float, ALLOCATOR, Sequence<3072>> encoder_layer_10_intermediate_dense_bias = Tensor<float, ALLOCATOR, Sequence<3072>>(bin, 94187520ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_10_output_dense_bias = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 94199808ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_10_output_LayerNorm_weight = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 94202880ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_10_output_LayerNorm_bias = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 94205952ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_11_attention_self_query_bias = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 94209024ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_11_attention_self_key_bias = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 94212096ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_11_attention_self_value_bias = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 94215168ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_11_attention_output_dense_bias = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 94218240ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_11_attention_output_LayerNorm_weight = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 94221312ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_11_attention_output_LayerNorm_bias = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 94224384ULL);
    Tensor<float, ALLOCATOR, Sequence<3072>> encoder_layer_11_intermediate_dense_bias = Tensor<float, ALLOCATOR, Sequence<3072>>(bin, 94227456ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_11_output_dense_bias = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 94239744ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_11_output_LayerNorm_weight = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 94242816ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> encoder_layer_11_output_LayerNorm_bias = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 94245888ULL);
    Tensor<float, ALLOCATOR, Sequence<768,768>> pooler_dense_weight = Tensor<float, ALLOCATOR, Sequence<768,768>>(bin, 94248960ULL);
    Tensor<float, ALLOCATOR, Sequence<768>> pooler_dense_bias = Tensor<float, ALLOCATOR, Sequence<768>>(bin, 96608256ULL);
    static constexpr int64_t _Constant_output_0 = 0;
    static constexpr float _Constant_18_output_0 = 1.0;
    static constexpr float _encoder_layer_0_intermediate_intermediate_act_fn_Constant_output_0 = 1.4142135381698608;
    static constexpr float _encoder_layer_0_intermediate_intermediate_act_fn_Constant_2_output_0 = 0.5;
    Tensor<float, ALLOCATOR, Sequence<1,512,768>> _embeddings_token_type_embeddings_Gather_output_0 = Tensor<float, ALLOCATOR, Sequence<1,512,768>>(bin, 96611328ULL);
    Tensor<float, ALLOCATOR, Sequence<1,512,768>> _embeddings_position_embeddings_Gather_output_0 = Tensor<float, ALLOCATOR, Sequence<1,512,768>>(bin, 98184192ULL);
    Tensor<float, ALLOCATOR, Sequence<768,768>> _encoder_layer_0_attention_self_query_Transpose_output_0 = Tensor<float, ALLOCATOR, Sequence<768,768>>(bin, 99757056ULL);
    Tensor<int64_t, ALLOCATOR, Sequence<4>> _encoder_layer_0_attention_self_Concat_output_0 = Tensor<int64_t, ALLOCATOR, Sequence<4>>(bin, 102116352ULL);
    Tensor<float, ALLOCATOR, Sequence<768,768>> _encoder_layer_0_attention_self_key_Transpose_output_0 = Tensor<float, ALLOCATOR, Sequence<768,768>>(bin, 102116384ULL);
    Tensor<float, ALLOCATOR, Sequence<768,768>> _encoder_layer_0_attention_self_value_Transpose_output_0 = Tensor<float, ALLOCATOR, Sequence<768,768>>(bin, 104475680ULL);
    Tensor<int64_t, ALLOCATOR, Sequence<3>> _encoder_layer_0_attention_self_Concat_3_output_0 = Tensor<int64_t, ALLOCATOR, Sequence<3>>(bin, 106834976ULL);
    Tensor<float, ALLOCATOR, Sequence<768,768>> _encoder_layer_0_attention_output_dense_Transpose_output_0 = Tensor<float, ALLOCATOR, Sequence<768,768>>(bin, 106835000ULL);
    Tensor<float, ALLOCATOR, Sequence<768,3072>> _encoder_layer_0_intermediate_dense_Transpose_output_0 = Tensor<float, ALLOCATOR, Sequence<768,3072>>(bin, 109194296ULL);
    Tensor<float, ALLOCATOR, Sequence<3072,768>> _encoder_layer_0_output_dense_Transpose_output_0 = Tensor<float, ALLOCATOR, Sequence<3072,768>>(bin, 118631480ULL);
    Tensor<float, ALLOCATOR, Sequence<768,768>> _encoder_layer_1_attention_self_query_Transpose_output_0 = Tensor<float, ALLOCATOR, Sequence<768,768>>(bin, 128068664ULL);
    Tensor<float, ALLOCATOR, Sequence<768,768>> _encoder_layer_1_attention_self_key_Transpose_output_0 = Tensor<float, ALLOCATOR, Sequence<768,768>>(bin, 130427960ULL);
    Tensor<float, ALLOCATOR, Sequence<768,768>> _encoder_layer_1_attention_self_value_Transpose_output_0 = Tensor<float, ALLOCATOR, Sequence<768,768>>(bin, 132787256ULL);
    Tensor<float, ALLOCATOR, Sequence<768,768>> _encoder_layer_1_attention_output_dense_Transpose_output_0 = Tensor<float, ALLOCATOR, Sequence<768,768>>(bin, 135146552ULL);
    Tensor<float, ALLOCATOR, Sequence<768,3072>> _encoder_layer_1_intermediate_dense_Transpose_output_0 = Tensor<float, ALLOCATOR, Sequence<768,3072>>(bin, 137505848ULL);
    Tensor<float, ALLOCATOR, Sequence<3072,768>> _encoder_layer_1_output_dense_Transpose_output_0 = Tensor<float, ALLOCATOR, Sequence<3072,768>>(bin, 146943032ULL);
    Tensor<float, ALLOCATOR, Sequence<768,768>> _encoder_layer_2_attention_self_query_Transpose_output_0 = Tensor<float, ALLOCATOR, Sequence<768,768>>(bin, 156380216ULL);
    Tensor<float, ALLOCATOR, Sequence<768,768>> _encoder_layer_2_attention_self_key_Transpose_output_0 = Tensor<float, ALLOCATOR, Sequence<768,768>>(bin, 158739512ULL);
    Tensor<float, ALLOCATOR, Sequence<768,768>> _encoder_layer_2_attention_self_value_Transpose_output_0 = Tensor<float, ALLOCATOR, Sequence<768,768>>(bin, 161098808ULL);
    Tensor<float, ALLOCATOR, Sequence<768,768>> _encoder_layer_2_attention_output_dense_Transpose_output_0 = Tensor<float, ALLOCATOR, Sequence<768,768>>(bin, 163458104ULL);
    Tensor<float, ALLOCATOR, Sequence<768,3072>> _encoder_layer_2_intermediate_dense_Transpose_output_0 = Tensor<float, ALLOCATOR, Sequence<768,3072>>(bin, 165817400ULL);
    Tensor<float, ALLOCATOR, Sequence<3072,768>> _encoder_layer_2_output_dense_Transpose_output_0 = Tensor<float, ALLOCATOR, Sequence<3072,768>>(bin, 175254584ULL);
    Tensor<float, ALLOCATOR, Sequence<768,768>> _encoder_layer_3_attention_self_query_Transpose_output_0 = Tensor<float, ALLOCATOR, Sequence<768,768>>(bin, 184691768ULL);
    Tensor<float, ALLOCATOR, Sequence<768,768>> _encoder_layer_3_attention_self_key_Transpose_output_0 = Tensor<float, ALLOCATOR, Sequence<768,768>>(bin, 187051064ULL);
    Tensor<float, ALLOCATOR, Sequence<768,768>> _encoder_layer_3_attention_self_value_Transpose_output_0 = Tensor<float, ALLOCATOR, Sequence<768,768>>(bin, 189410360ULL);
    Tensor<float, ALLOCATOR, Sequence<768,768>> _encoder_layer_3_attention_output_dense_Transpose_output_0 = Tensor<float, ALLOCATOR, Sequence<768,768>>(bin, 191769656ULL);
    Tensor<float, ALLOCATOR, Sequence<768,3072>> _encoder_layer_3_intermediate_dense_Transpose_output_0 = Tensor<float, ALLOCATOR, Sequence<768,3072>>(bin, 194128952ULL);
    Tensor<float, ALLOCATOR, Sequence<3072,768>> _encoder_layer_3_output_dense_Transpose_output_0 = Tensor<float, ALLOCATOR, Sequence<3072,768>>(bin, 203566136ULL);
    Tensor<float, ALLOCATOR, Sequence<768,768>> _encoder_layer_4_attention_self_query_Transpose_output_0 = Tensor<float, ALLOCATOR, Sequence<768,768>>(bin, 213003320ULL);
    Tensor<float, ALLOCATOR, Sequence<768,768>> _encoder_layer_4_attention_self_key_Transpose_output_0 = Tensor<float, ALLOCATOR, Sequence<768,768>>(bin, 215362616ULL);
    Tensor<float, ALLOCATOR, Sequence<768,768>> _encoder_layer_4_attention_self_value_Transpose_output_0 = Tensor<float, ALLOCATOR, Sequence<768,768>>(bin, 217721912ULL);
    Tensor<float, ALLOCATOR, Sequence<768,768>> _encoder_layer_4_attention_output_dense_Transpose_output_0 = Tensor<float, ALLOCATOR, Sequence<768,768>>(bin, 220081208ULL);
    Tensor<float, ALLOCATOR, Sequence<768,3072>> _encoder_layer_4_intermediate_dense_Transpose_output_0 = Tensor<float, ALLOCATOR, Sequence<768,3072>>(bin, 222440504ULL);
    Tensor<float, ALLOCATOR, Sequence<3072,768>> _encoder_layer_4_output_dense_Transpose_output_0 = Tensor<float, ALLOCATOR, Sequence<3072,768>>(bin, 231877688ULL);
    Tensor<float, ALLOCATOR, Sequence<768,768>> _encoder_layer_5_attention_self_query_Transpose_output_0 = Tensor<float, ALLOCATOR, Sequence<768,768>>(bin, 241314872ULL);
    Tensor<float, ALLOCATOR, Sequence<768,768>> _encoder_layer_5_attention_self_key_Transpose_output_0 = Tensor<float, ALLOCATOR, Sequence<768,768>>(bin, 243674168ULL);
    Tensor<float, ALLOCATOR, Sequence<768,768>> _encoder_layer_5_attention_self_value_Transpose_output_0 = Tensor<float, ALLOCATOR, Sequence<768,768>>(bin, 246033464ULL);
    Tensor<float, ALLOCATOR, Sequence<768,768>> _encoder_layer_5_attention_output_dense_Transpose_output_0 = Tensor<float, ALLOCATOR, Sequence<768,768>>(bin, 248392760ULL);
    Tensor<float, ALLOCATOR, Sequence<768,3072>> _encoder_layer_5_intermediate_dense_Transpose_output_0 = Tensor<float, ALLOCATOR, Sequence<768,3072>>(bin, 250752056ULL);
    Tensor<float, ALLOCATOR, Sequence<3072,768>> _encoder_layer_5_output_dense_Transpose_output_0 = Tensor<float, ALLOCATOR, Sequence<3072,768>>(bin, 260189240ULL);
    Tensor<float, ALLOCATOR, Sequence<768,768>> _encoder_layer_6_attention_self_query_Transpose_output_0 = Tensor<float, ALLOCATOR, Sequence<768,768>>(bin, 269626424ULL);
    Tensor<float, ALLOCATOR, Sequence<768,768>> _encoder_layer_6_attention_self_key_Transpose_output_0 = Tensor<float, ALLOCATOR, Sequence<768,768>>(bin, 271985720ULL);
    Tensor<float, ALLOCATOR, Sequence<768,768>> _encoder_layer_6_attention_self_value_Transpose_output_0 = Tensor<float, ALLOCATOR, Sequence<768,768>>(bin, 274345016ULL);
    Tensor<float, ALLOCATOR, Sequence<768,768>> _encoder_layer_6_attention_output_dense_Transpose_output_0 = Tensor<float, ALLOCATOR, Sequence<768,768>>(bin, 276704312ULL);
    Tensor<float, ALLOCATOR, Sequence<768,3072>> _encoder_layer_6_intermediate_dense_Transpose_output_0 = Tensor<float, ALLOCATOR, Sequence<768,3072>>(bin, 279063608ULL);
    Tensor<float, ALLOCATOR, Sequence<3072,768>> _encoder_layer_6_output_dense_Transpose_output_0 = Tensor<float, ALLOCATOR, Sequence<3072,768>>(bin, 288500792ULL);
    Tensor<float, ALLOCATOR, Sequence<768,768>> _encoder_layer_7_attention_self_query_Transpose_output_0 = Tensor<float, ALLOCATOR, Sequence<768,768>>(bin, 297937976ULL);
    Tensor<float, ALLOCATOR, Sequence<768,768>> _encoder_layer_7_attention_self_key_Transpose_output_0 = Tensor<float, ALLOCATOR, Sequence<768,768>>(bin, 300297272ULL);
    Tensor<float, ALLOCATOR, Sequence<768,768>> _encoder_layer_7_attention_self_value_Transpose_output_0 = Tensor<float, ALLOCATOR, Sequence<768,768>>(bin, 302656568ULL);
    Tensor<float, ALLOCATOR, Sequence<768,768>> _encoder_layer_7_attention_output_dense_Transpose_output_0 = Tensor<float, ALLOCATOR, Sequence<768,768>>(bin, 305015864ULL);
    Tensor<float, ALLOCATOR, Sequence<768,3072>> _encoder_layer_7_intermediate_dense_Transpose_output_0 = Tensor<float, ALLOCATOR, Sequence<768,3072>>(bin, 307375160ULL);
    Tensor<float, ALLOCATOR, Sequence<3072,768>> _encoder_layer_7_output_dense_Transpose_output_0 = Tensor<float, ALLOCATOR, Sequence<3072,768>>(bin, 316812344ULL);
    Tensor<float, ALLOCATOR, Sequence<768,768>> _encoder_layer_8_attention_self_query_Transpose_output_0 = Tensor<float, ALLOCATOR, Sequence<768,768>>(bin, 326249528ULL);
    Tensor<float, ALLOCATOR, Sequence<768,768>> _encoder_layer_8_attention_self_key_Transpose_output_0 = Tensor<float, ALLOCATOR, Sequence<768,768>>(bin, 328608824ULL);
    Tensor<float, ALLOCATOR, Sequence<768,768>> _encoder_layer_8_attention_self_value_Transpose_output_0 = Tensor<float, ALLOCATOR, Sequence<768,768>>(bin, 330968120ULL);
    Tensor<float, ALLOCATOR, Sequence<768,768>> _encoder_layer_8_attention_output_dense_Transpose_output_0 = Tensor<float, ALLOCATOR, Sequence<768,768>>(bin, 333327416ULL);
    Tensor<float, ALLOCATOR, Sequence<768,3072>> _encoder_layer_8_intermediate_dense_Transpose_output_0 = Tensor<float, ALLOCATOR, Sequence<768,3072>>(bin, 335686712ULL);
    Tensor<float, ALLOCATOR, Sequence<3072,768>> _encoder_layer_8_output_dense_Transpose_output_0 = Tensor<float, ALLOCATOR, Sequence<3072,768>>(bin, 345123896ULL);
    Tensor<float, ALLOCATOR, Sequence<768,768>> _encoder_layer_9_attention_self_query_Transpose_output_0 = Tensor<float, ALLOCATOR, Sequence<768,768>>(bin, 354561080ULL);
    Tensor<float, ALLOCATOR, Sequence<768,768>> _encoder_layer_9_attention_self_key_Transpose_output_0 = Tensor<float, ALLOCATOR, Sequence<768,768>>(bin, 356920376ULL);
    Tensor<float, ALLOCATOR, Sequence<768,768>> _encoder_layer_9_attention_self_value_Transpose_output_0 = Tensor<float, ALLOCATOR, Sequence<768,768>>(bin, 359279672ULL);
    Tensor<float, ALLOCATOR, Sequence<768,768>> _encoder_layer_9_attention_output_dense_Transpose_output_0 = Tensor<float, ALLOCATOR, Sequence<768,768>>(bin, 361638968ULL);
    Tensor<float, ALLOCATOR, Sequence<768,3072>> _encoder_layer_9_intermediate_dense_Transpose_output_0 = Tensor<float, ALLOCATOR, Sequence<768,3072>>(bin, 363998264ULL);
    Tensor<float, ALLOCATOR, Sequence<3072,768>> _encoder_layer_9_output_dense_Transpose_output_0 = Tensor<float, ALLOCATOR, Sequence<3072,768>>(bin, 373435448ULL);
    Tensor<float, ALLOCATOR, Sequence<768,768>> _encoder_layer_10_attention_self_query_Transpose_output_0 = Tensor<float, ALLOCATOR, Sequence<768,768>>(bin, 382872632ULL);
    Tensor<float, ALLOCATOR, Sequence<768,768>> _encoder_layer_10_attention_self_key_Transpose_output_0 = Tensor<float, ALLOCATOR, Sequence<768,768>>(bin, 385231928ULL);
    Tensor<float, ALLOCATOR, Sequence<768,768>> _encoder_layer_10_attention_self_value_Transpose_output_0 = Tensor<float, ALLOCATOR, Sequence<768,768>>(bin, 387591224ULL);
    Tensor<float, ALLOCATOR, Sequence<768,768>> _encoder_layer_10_attention_output_dense_Transpose_output_0 = Tensor<float, ALLOCATOR, Sequence<768,768>>(bin, 389950520ULL);
    Tensor<float, ALLOCATOR, Sequence<768,3072>> _encoder_layer_10_intermediate_dense_Transpose_output_0 = Tensor<float, ALLOCATOR, Sequence<768,3072>>(bin, 392309816ULL);
    Tensor<float, ALLOCATOR, Sequence<3072,768>> _encoder_layer_10_output_dense_Transpose_output_0 = Tensor<float, ALLOCATOR, Sequence<3072,768>>(bin, 401747000ULL);
    Tensor<float, ALLOCATOR, Sequence<768,768>> _encoder_layer_11_attention_self_query_Transpose_output_0 = Tensor<float, ALLOCATOR, Sequence<768,768>>(bin, 411184184ULL);
    Tensor<float, ALLOCATOR, Sequence<768,768>> _encoder_layer_11_attention_self_key_Transpose_output_0 = Tensor<float, ALLOCATOR, Sequence<768,768>>(bin, 413543480ULL);
    Tensor<float, ALLOCATOR, Sequence<768,768>> _encoder_layer_11_attention_self_value_Transpose_output_0 = Tensor<float, ALLOCATOR, Sequence<768,768>>(bin, 415902776ULL);
    Tensor<float, ALLOCATOR, Sequence<768,768>> _encoder_layer_11_attention_output_dense_Transpose_output_0 = Tensor<float, ALLOCATOR, Sequence<768,768>>(bin, 418262072ULL);
    Tensor<float, ALLOCATOR, Sequence<768,3072>> _encoder_layer_11_intermediate_dense_Transpose_output_0 = Tensor<float, ALLOCATOR, Sequence<768,3072>>(bin, 420621368ULL);
    Tensor<float, ALLOCATOR, Sequence<3072,768>> _encoder_layer_11_output_dense_Transpose_output_0 = Tensor<float, ALLOCATOR, Sequence<3072,768>>(bin, 430058552ULL);
    Tensor<float, ALLOCATOR, Sequence<1>> _encoder_layer_0_attention_self_Sqrt_1_output_0 = Tensor<float, ALLOCATOR, Sequence<1>>(bin, 439495736ULL);

Model(const std::string& weights) : bin(weights) {}

auto forward(Tensor<int32_t, ALLOCATOR, Sequence<1,512>> input_1) {

    auto _embeddings_word_embeddings_Gather_output_0 = gather(embeddings_word_embeddings_weight, input_1);
    auto _embeddings_Add_output_0 = _embeddings_word_embeddings_Gather_output_0 + _embeddings_token_type_embeddings_Gather_output_0;
    auto _embeddings_Add_1_output_0 = _embeddings_Add_output_0 + _embeddings_position_embeddings_Gather_output_0;
    auto _embeddings_LayerNorm_LayerNormalization_output_0 = layer_norm<-1>(_embeddings_Add_1_output_0, embeddings_LayerNorm_weight, embeddings_LayerNorm_bias, 9.999999960041972e-13);
    auto _encoder_layer_0_attention_self_query_MatMul_output_0 = matmul<1>(_embeddings_LayerNorm_LayerNormalization_output_0, _encoder_layer_0_attention_self_query_Transpose_output_0);
    auto _encoder_layer_0_attention_self_query_Add_output_0 = encoder_layer_0_attention_self_query_bias + _encoder_layer_0_attention_self_query_MatMul_output_0;
    auto _encoder_layer_0_attention_self_Reshape_output_0 = _encoder_layer_0_attention_self_query_Add_output_0.reshape<1,512,12,64>();
    auto _encoder_layer_0_attention_self_Transpose_output_0 = _encoder_layer_0_attention_self_Reshape_output_0.transpose<0,2,1,3>();
    auto _encoder_layer_0_attention_self_key_MatMul_output_0 = matmul<1>(_embeddings_LayerNorm_LayerNormalization_output_0, _encoder_layer_0_attention_self_key_Transpose_output_0);
    auto _encoder_layer_0_attention_self_key_Add_output_0 = encoder_layer_0_attention_self_key_bias + _encoder_layer_0_attention_self_key_MatMul_output_0;
    auto _encoder_layer_0_attention_self_Reshape_1_output_0 = _encoder_layer_0_attention_self_key_Add_output_0.reshape<1,512,12,64>();
    auto _encoder_layer_0_attention_self_value_MatMul_output_0 = matmul<1>(_embeddings_LayerNorm_LayerNormalization_output_0, _encoder_layer_0_attention_self_value_Transpose_output_0);
    auto _encoder_layer_0_attention_self_value_Add_output_0 = encoder_layer_0_attention_self_value_bias + _encoder_layer_0_attention_self_value_MatMul_output_0;
    auto _encoder_layer_0_attention_self_Reshape_2_output_0 = _encoder_layer_0_attention_self_value_Add_output_0.reshape<1,512,12,64>();
    auto _encoder_layer_0_attention_self_Transpose_1_output_0 = _encoder_layer_0_attention_self_Reshape_2_output_0.transpose<0,2,1,3>();
    auto _encoder_layer_0_attention_self_Transpose_2_output_0 = _encoder_layer_0_attention_self_Reshape_1_output_0.transpose<0,2,3,1>();
    auto _encoder_layer_0_attention_self_Mul_output_0 = _encoder_layer_0_attention_self_Transpose_output_0 * _encoder_layer_0_attention_self_Sqrt_1_output_0;
    auto _encoder_layer_0_attention_self_Mul_1_output_0 = _encoder_layer_0_attention_self_Transpose_2_output_0 * _encoder_layer_0_attention_self_Sqrt_1_output_0;
    auto _encoder_layer_0_attention_self_MatMul_output_0 = matmul<1>(_encoder_layer_0_attention_self_Mul_output_0, _encoder_layer_0_attention_self_Mul_1_output_0);
    auto _encoder_layer_0_attention_self_Softmax_output_0 = softmax<-1, true>(_encoder_layer_0_attention_self_MatMul_output_0);
    auto _encoder_layer_0_attention_self_MatMul_1_output_0 = matmul<1>(_encoder_layer_0_attention_self_Softmax_output_0, _encoder_layer_0_attention_self_Transpose_1_output_0);

    auto _encoder_layer_0_attention_self_Transpose_3_output_0 = _encoder_layer_0_attention_self_MatMul_1_output_0.transpose<0,2,1,3>();
    auto _encoder_layer_0_attention_self_Reshape_3_output_0 = _encoder_layer_0_attention_self_Transpose_3_output_0.reshape<1,512,768>();
    auto _encoder_layer_0_attention_output_dense_MatMul_output_0 = matmul<1>(_encoder_layer_0_attention_self_Reshape_3_output_0, _encoder_layer_0_attention_output_dense_Transpose_output_0);
    auto _encoder_layer_0_attention_output_dense_Add_output_0 = encoder_layer_0_attention_output_dense_bias + _encoder_layer_0_attention_output_dense_MatMul_output_0;
    auto _encoder_layer_0_attention_output_Add_output_0 = _encoder_layer_0_attention_output_dense_Add_output_0 + _embeddings_LayerNorm_LayerNormalization_output_0;
    auto _encoder_layer_0_attention_output_LayerNorm_LayerNormalization_output_0 = layer_norm<-1>(_encoder_layer_0_attention_output_Add_output_0, encoder_layer_0_attention_output_LayerNorm_weight, encoder_layer_0_attention_output_LayerNorm_bias, 9.999999960041972e-13);
    auto _encoder_layer_0_intermediate_dense_MatMul_output_0 = matmul<1>(_encoder_layer_0_attention_output_LayerNorm_LayerNormalization_output_0, _encoder_layer_0_intermediate_dense_Transpose_output_0);
    auto _encoder_layer_0_intermediate_dense_Add_output_0 = encoder_layer_0_intermediate_dense_bias + _encoder_layer_0_intermediate_dense_MatMul_output_0;
    auto _encoder_layer_0_intermediate_intermediate_act_fn_Div_output_0 = _encoder_layer_0_intermediate_dense_Add_output_0 / _encoder_layer_0_intermediate_intermediate_act_fn_Constant_output_0;
    auto _encoder_layer_0_intermediate_intermediate_act_fn_Erf_output_0 = erf<true>(_encoder_layer_0_intermediate_intermediate_act_fn_Div_output_0);
    auto _encoder_layer_0_intermediate_intermediate_act_fn_Add_output_0 = _encoder_layer_0_intermediate_intermediate_act_fn_Erf_output_0 + _Constant_18_output_0;
    auto _encoder_layer_0_intermediate_intermediate_act_fn_Mul_output_0 = _encoder_layer_0_intermediate_dense_Add_output_0 * _encoder_layer_0_intermediate_intermediate_act_fn_Add_output_0;
    auto _encoder_layer_0_intermediate_intermediate_act_fn_Mul_1_output_0 = _encoder_layer_0_intermediate_intermediate_act_fn_Mul_output_0 * _encoder_layer_0_intermediate_intermediate_act_fn_Constant_2_output_0;
    auto _encoder_layer_0_output_dense_MatMul_output_0 = matmul<1>(_encoder_layer_0_intermediate_intermediate_act_fn_Mul_1_output_0, _encoder_layer_0_output_dense_Transpose_output_0);
    auto _encoder_layer_0_output_dense_Add_output_0 = encoder_layer_0_output_dense_bias + _encoder_layer_0_output_dense_MatMul_output_0;
    auto _encoder_layer_0_output_Add_output_0 = _encoder_layer_0_output_dense_Add_output_0 + _encoder_layer_0_attention_output_LayerNorm_LayerNormalization_output_0;
    auto _encoder_layer_0_output_LayerNorm_LayerNormalization_output_0 = layer_norm<-1>(_encoder_layer_0_output_Add_output_0, encoder_layer_0_output_LayerNorm_weight, encoder_layer_0_output_LayerNorm_bias, 9.999999960041972e-13);
    auto _encoder_layer_1_attention_self_query_MatMul_output_0 = matmul<1>(_encoder_layer_0_output_LayerNorm_LayerNormalization_output_0, _encoder_layer_1_attention_self_query_Transpose_output_0);
    auto _encoder_layer_1_attention_self_query_Add_output_0 = encoder_layer_1_attention_self_query_bias + _encoder_layer_1_attention_self_query_MatMul_output_0;
    auto _encoder_layer_1_attention_self_Reshape_output_0 = _encoder_layer_1_attention_self_query_Add_output_0.reshape<1,512,12,64>();
    auto _encoder_layer_1_attention_self_Transpose_output_0 = _encoder_layer_1_attention_self_Reshape_output_0.transpose<0,2,1,3>();
    auto _encoder_layer_1_attention_self_key_MatMul_output_0 = matmul<1>(_encoder_layer_0_output_LayerNorm_LayerNormalization_output_0, _encoder_layer_1_attention_self_key_Transpose_output_0);
    auto _encoder_layer_1_attention_self_key_Add_output_0 = encoder_layer_1_attention_self_key_bias + _encoder_layer_1_attention_self_key_MatMul_output_0;
    auto _encoder_layer_1_attention_self_Reshape_1_output_0 = _encoder_layer_1_attention_self_key_Add_output_0.reshape<1,512,12,64>();
    auto _encoder_layer_1_attention_self_value_MatMul_output_0 = matmul<1>(_encoder_layer_0_output_LayerNorm_LayerNormalization_output_0, _encoder_layer_1_attention_self_value_Transpose_output_0);
    auto _encoder_layer_1_attention_self_value_Add_output_0 = encoder_layer_1_attention_self_value_bias + _encoder_layer_1_attention_self_value_MatMul_output_0;
    auto _encoder_layer_1_attention_self_Reshape_2_output_0 = _encoder_layer_1_attention_self_value_Add_output_0.reshape<1,512,12,64>();
    auto _encoder_layer_1_attention_self_Transpose_1_output_0 = _encoder_layer_1_attention_self_Reshape_2_output_0.transpose<0,2,1,3>();
    auto _encoder_layer_1_attention_self_Transpose_2_output_0 = _encoder_layer_1_attention_self_Reshape_1_output_0.transpose<0,2,3,1>();
    auto _encoder_layer_1_attention_self_Mul_output_0 = _encoder_layer_1_attention_self_Transpose_output_0 * _encoder_layer_0_attention_self_Sqrt_1_output_0;
    auto _encoder_layer_1_attention_self_Mul_1_output_0 = _encoder_layer_1_attention_self_Transpose_2_output_0 * _encoder_layer_0_attention_self_Sqrt_1_output_0;
    auto _encoder_layer_1_attention_self_MatMul_output_0 = matmul<1>(_encoder_layer_1_attention_self_Mul_output_0, _encoder_layer_1_attention_self_Mul_1_output_0);
    auto _encoder_layer_1_attention_self_Softmax_output_0 = softmax<-1, true>(_encoder_layer_1_attention_self_MatMul_output_0);
    auto _encoder_layer_1_attention_self_MatMul_1_output_0 = matmul<1>(_encoder_layer_1_attention_self_Softmax_output_0, _encoder_layer_1_attention_self_Transpose_1_output_0);
    auto _encoder_layer_1_attention_self_Transpose_3_output_0 = _encoder_layer_1_attention_self_MatMul_1_output_0.transpose<0,2,1,3>();
    auto _encoder_layer_1_attention_self_Reshape_3_output_0 = _encoder_layer_1_attention_self_Transpose_3_output_0.reshape<1,512,768>();
    auto _encoder_layer_1_attention_output_dense_MatMul_output_0 = matmul<1>(_encoder_layer_1_attention_self_Reshape_3_output_0, _encoder_layer_1_attention_output_dense_Transpose_output_0);
    auto _encoder_layer_1_attention_output_dense_Add_output_0 = encoder_layer_1_attention_output_dense_bias + _encoder_layer_1_attention_output_dense_MatMul_output_0;
    auto _encoder_layer_1_attention_output_Add_output_0 = _encoder_layer_1_attention_output_dense_Add_output_0 + _encoder_layer_0_output_LayerNorm_LayerNormalization_output_0;
    auto _encoder_layer_1_attention_output_LayerNorm_LayerNormalization_output_0 = layer_norm<-1>(_encoder_layer_1_attention_output_Add_output_0, encoder_layer_1_attention_output_LayerNorm_weight, encoder_layer_1_attention_output_LayerNorm_bias, 9.999999960041972e-13);
    auto _encoder_layer_1_intermediate_dense_MatMul_output_0 = matmul<1>(_encoder_layer_1_attention_output_LayerNorm_LayerNormalization_output_0, _encoder_layer_1_intermediate_dense_Transpose_output_0);
    auto _encoder_layer_1_intermediate_dense_Add_output_0 = encoder_layer_1_intermediate_dense_bias + _encoder_layer_1_intermediate_dense_MatMul_output_0;
    auto _encoder_layer_1_intermediate_intermediate_act_fn_Div_output_0 = _encoder_layer_1_intermediate_dense_Add_output_0 / _encoder_layer_0_intermediate_intermediate_act_fn_Constant_output_0;
    auto _encoder_layer_1_intermediate_intermediate_act_fn_Erf_output_0 = erf<true>(_encoder_layer_1_intermediate_intermediate_act_fn_Div_output_0);
    auto _encoder_layer_1_intermediate_intermediate_act_fn_Add_output_0 = _encoder_layer_1_intermediate_intermediate_act_fn_Erf_output_0 + _Constant_18_output_0;
    auto _encoder_layer_1_intermediate_intermediate_act_fn_Mul_output_0 = _encoder_layer_1_intermediate_dense_Add_output_0 * _encoder_layer_1_intermediate_intermediate_act_fn_Add_output_0;
    auto _encoder_layer_1_intermediate_intermediate_act_fn_Mul_1_output_0 = _encoder_layer_1_intermediate_intermediate_act_fn_Mul_output_0 * _encoder_layer_0_intermediate_intermediate_act_fn_Constant_2_output_0;
    auto _encoder_layer_1_output_dense_MatMul_output_0 = matmul<1>(_encoder_layer_1_intermediate_intermediate_act_fn_Mul_1_output_0, _encoder_layer_1_output_dense_Transpose_output_0);
    auto _encoder_layer_1_output_dense_Add_output_0 = encoder_layer_1_output_dense_bias + _encoder_layer_1_output_dense_MatMul_output_0;
    auto _encoder_layer_1_output_Add_output_0 = _encoder_layer_1_output_dense_Add_output_0 + _encoder_layer_1_attention_output_LayerNorm_LayerNormalization_output_0;
    auto _encoder_layer_1_output_LayerNorm_LayerNormalization_output_0 = layer_norm<-1>(_encoder_layer_1_output_Add_output_0, encoder_layer_1_output_LayerNorm_weight, encoder_layer_1_output_LayerNorm_bias, 9.999999960041972e-13);
    auto _encoder_layer_2_attention_self_query_MatMul_output_0 = matmul<1>(_encoder_layer_1_output_LayerNorm_LayerNormalization_output_0, _encoder_layer_2_attention_self_query_Transpose_output_0);
    auto _encoder_layer_2_attention_self_query_Add_output_0 = encoder_layer_2_attention_self_query_bias + _encoder_layer_2_attention_self_query_MatMul_output_0;
    auto _encoder_layer_2_attention_self_Reshape_output_0 = _encoder_layer_2_attention_self_query_Add_output_0.reshape<1,512,12,64>();
    auto _encoder_layer_2_attention_self_Transpose_output_0 = _encoder_layer_2_attention_self_Reshape_output_0.transpose<0,2,1,3>();
    auto _encoder_layer_2_attention_self_key_MatMul_output_0 = matmul<1>(_encoder_layer_1_output_LayerNorm_LayerNormalization_output_0, _encoder_layer_2_attention_self_key_Transpose_output_0);
    auto _encoder_layer_2_attention_self_key_Add_output_0 = encoder_layer_2_attention_self_key_bias + _encoder_layer_2_attention_self_key_MatMul_output_0;
    auto _encoder_layer_2_attention_self_Reshape_1_output_0 = _encoder_layer_2_attention_self_key_Add_output_0.reshape<1,512,12,64>();
    auto _encoder_layer_2_attention_self_value_MatMul_output_0 = matmul<1>(_encoder_layer_1_output_LayerNorm_LayerNormalization_output_0, _encoder_layer_2_attention_self_value_Transpose_output_0);
    auto _encoder_layer_2_attention_self_value_Add_output_0 = encoder_layer_2_attention_self_value_bias + _encoder_layer_2_attention_self_value_MatMul_output_0;
    auto _encoder_layer_2_attention_self_Reshape_2_output_0 = _encoder_layer_2_attention_self_value_Add_output_0.reshape<1,512,12,64>();
    auto _encoder_layer_2_attention_self_Transpose_1_output_0 = _encoder_layer_2_attention_self_Reshape_2_output_0.transpose<0,2,1,3>();
    auto _encoder_layer_2_attention_self_Transpose_2_output_0 = _encoder_layer_2_attention_self_Reshape_1_output_0.transpose<0,2,3,1>();
    auto _encoder_layer_2_attention_self_Mul_output_0 = _encoder_layer_2_attention_self_Transpose_output_0 * _encoder_layer_0_attention_self_Sqrt_1_output_0;
    auto _encoder_layer_2_attention_self_Mul_1_output_0 = _encoder_layer_2_attention_self_Transpose_2_output_0 * _encoder_layer_0_attention_self_Sqrt_1_output_0;
    auto _encoder_layer_2_attention_self_MatMul_output_0 = matmul<1>(_encoder_layer_2_attention_self_Mul_output_0, _encoder_layer_2_attention_self_Mul_1_output_0);
    auto _encoder_layer_2_attention_self_Softmax_output_0 = softmax<-1, true>(_encoder_layer_2_attention_self_MatMul_output_0);
    auto _encoder_layer_2_attention_self_MatMul_1_output_0 = matmul<1>(_encoder_layer_2_attention_self_Softmax_output_0, _encoder_layer_2_attention_self_Transpose_1_output_0);
    auto _encoder_layer_2_attention_self_Transpose_3_output_0 = _encoder_layer_2_attention_self_MatMul_1_output_0.transpose<0,2,1,3>();
    auto _encoder_layer_2_attention_self_Reshape_3_output_0 = _encoder_layer_2_attention_self_Transpose_3_output_0.reshape<1,512,768>();
    auto _encoder_layer_2_attention_output_dense_MatMul_output_0 = matmul<1>(_encoder_layer_2_attention_self_Reshape_3_output_0, _encoder_layer_2_attention_output_dense_Transpose_output_0);
    auto _encoder_layer_2_attention_output_dense_Add_output_0 = encoder_layer_2_attention_output_dense_bias + _encoder_layer_2_attention_output_dense_MatMul_output_0;
    auto _encoder_layer_2_attention_output_Add_output_0 = _encoder_layer_2_attention_output_dense_Add_output_0 + _encoder_layer_1_output_LayerNorm_LayerNormalization_output_0;
    auto _encoder_layer_2_attention_output_LayerNorm_LayerNormalization_output_0 = layer_norm<-1>(_encoder_layer_2_attention_output_Add_output_0, encoder_layer_2_attention_output_LayerNorm_weight, encoder_layer_2_attention_output_LayerNorm_bias, 9.999999960041972e-13);
    auto _encoder_layer_2_intermediate_dense_MatMul_output_0 = matmul<1>(_encoder_layer_2_attention_output_LayerNorm_LayerNormalization_output_0, _encoder_layer_2_intermediate_dense_Transpose_output_0);
    auto _encoder_layer_2_intermediate_dense_Add_output_0 = encoder_layer_2_intermediate_dense_bias + _encoder_layer_2_intermediate_dense_MatMul_output_0;
    auto _encoder_layer_2_intermediate_intermediate_act_fn_Div_output_0 = _encoder_layer_2_intermediate_dense_Add_output_0 / _encoder_layer_0_intermediate_intermediate_act_fn_Constant_output_0;
    auto _encoder_layer_2_intermediate_intermediate_act_fn_Erf_output_0 = erf<true>(_encoder_layer_2_intermediate_intermediate_act_fn_Div_output_0);
    auto _encoder_layer_2_intermediate_intermediate_act_fn_Add_output_0 = _encoder_layer_2_intermediate_intermediate_act_fn_Erf_output_0 + _Constant_18_output_0;
    auto _encoder_layer_2_intermediate_intermediate_act_fn_Mul_output_0 = _encoder_layer_2_intermediate_dense_Add_output_0 * _encoder_layer_2_intermediate_intermediate_act_fn_Add_output_0;
    auto _encoder_layer_2_intermediate_intermediate_act_fn_Mul_1_output_0 = _encoder_layer_2_intermediate_intermediate_act_fn_Mul_output_0 * _encoder_layer_0_intermediate_intermediate_act_fn_Constant_2_output_0;
    auto _encoder_layer_2_output_dense_MatMul_output_0 = matmul<1>(_encoder_layer_2_intermediate_intermediate_act_fn_Mul_1_output_0, _encoder_layer_2_output_dense_Transpose_output_0);
    auto _encoder_layer_2_output_dense_Add_output_0 = encoder_layer_2_output_dense_bias + _encoder_layer_2_output_dense_MatMul_output_0;
    auto _encoder_layer_2_output_Add_output_0 = _encoder_layer_2_output_dense_Add_output_0 + _encoder_layer_2_attention_output_LayerNorm_LayerNormalization_output_0;
    auto _encoder_layer_2_output_LayerNorm_LayerNormalization_output_0 = layer_norm<-1>(_encoder_layer_2_output_Add_output_0, encoder_layer_2_output_LayerNorm_weight, encoder_layer_2_output_LayerNorm_bias, 9.999999960041972e-13);
    auto _encoder_layer_3_attention_self_query_MatMul_output_0 = matmul<1>(_encoder_layer_2_output_LayerNorm_LayerNormalization_output_0, _encoder_layer_3_attention_self_query_Transpose_output_0);
    auto _encoder_layer_3_attention_self_query_Add_output_0 = encoder_layer_3_attention_self_query_bias + _encoder_layer_3_attention_self_query_MatMul_output_0;
    auto _encoder_layer_3_attention_self_Reshape_output_0 = _encoder_layer_3_attention_self_query_Add_output_0.reshape<1,512,12,64>();
    auto _encoder_layer_3_attention_self_Transpose_output_0 = _encoder_layer_3_attention_self_Reshape_output_0.transpose<0,2,1,3>();
    auto _encoder_layer_3_attention_self_key_MatMul_output_0 = matmul<1>(_encoder_layer_2_output_LayerNorm_LayerNormalization_output_0, _encoder_layer_3_attention_self_key_Transpose_output_0);
    auto _encoder_layer_3_attention_self_key_Add_output_0 = encoder_layer_3_attention_self_key_bias + _encoder_layer_3_attention_self_key_MatMul_output_0;
    auto _encoder_layer_3_attention_self_Reshape_1_output_0 = _encoder_layer_3_attention_self_key_Add_output_0.reshape<1,512,12,64>();
    auto _encoder_layer_3_attention_self_value_MatMul_output_0 = matmul<1>(_encoder_layer_2_output_LayerNorm_LayerNormalization_output_0, _encoder_layer_3_attention_self_value_Transpose_output_0);
    auto _encoder_layer_3_attention_self_value_Add_output_0 = encoder_layer_3_attention_self_value_bias + _encoder_layer_3_attention_self_value_MatMul_output_0;
    auto _encoder_layer_3_attention_self_Reshape_2_output_0 = _encoder_layer_3_attention_self_value_Add_output_0.reshape<1,512,12,64>();
    auto _encoder_layer_3_attention_self_Transpose_1_output_0 = _encoder_layer_3_attention_self_Reshape_2_output_0.transpose<0,2,1,3>();
    auto _encoder_layer_3_attention_self_Transpose_2_output_0 = _encoder_layer_3_attention_self_Reshape_1_output_0.transpose<0,2,3,1>();
    auto _encoder_layer_3_attention_self_Mul_output_0 = _encoder_layer_3_attention_self_Transpose_output_0 * _encoder_layer_0_attention_self_Sqrt_1_output_0;
    auto _encoder_layer_3_attention_self_Mul_1_output_0 = _encoder_layer_3_attention_self_Transpose_2_output_0 * _encoder_layer_0_attention_self_Sqrt_1_output_0;
    auto _encoder_layer_3_attention_self_MatMul_output_0 = matmul<1>(_encoder_layer_3_attention_self_Mul_output_0, _encoder_layer_3_attention_self_Mul_1_output_0);
    auto _encoder_layer_3_attention_self_Softmax_output_0 = softmax<-1, true>(_encoder_layer_3_attention_self_MatMul_output_0);
    auto _encoder_layer_3_attention_self_MatMul_1_output_0 = matmul<1>(_encoder_layer_3_attention_self_Softmax_output_0, _encoder_layer_3_attention_self_Transpose_1_output_0);
    auto _encoder_layer_3_attention_self_Transpose_3_output_0 = _encoder_layer_3_attention_self_MatMul_1_output_0.transpose<0,2,1,3>();
    auto _encoder_layer_3_attention_self_Reshape_3_output_0 = _encoder_layer_3_attention_self_Transpose_3_output_0.reshape<1,512,768>();
    auto _encoder_layer_3_attention_output_dense_MatMul_output_0 = matmul<1>(_encoder_layer_3_attention_self_Reshape_3_output_0, _encoder_layer_3_attention_output_dense_Transpose_output_0);
    auto _encoder_layer_3_attention_output_dense_Add_output_0 = encoder_layer_3_attention_output_dense_bias + _encoder_layer_3_attention_output_dense_MatMul_output_0;
    auto _encoder_layer_3_attention_output_Add_output_0 = _encoder_layer_3_attention_output_dense_Add_output_0 + _encoder_layer_2_output_LayerNorm_LayerNormalization_output_0;
    auto _encoder_layer_3_attention_output_LayerNorm_LayerNormalization_output_0 = layer_norm<-1>(_encoder_layer_3_attention_output_Add_output_0, encoder_layer_3_attention_output_LayerNorm_weight, encoder_layer_3_attention_output_LayerNorm_bias, 9.999999960041972e-13);
    auto _encoder_layer_3_intermediate_dense_MatMul_output_0 = matmul<1>(_encoder_layer_3_attention_output_LayerNorm_LayerNormalization_output_0, _encoder_layer_3_intermediate_dense_Transpose_output_0);
    auto _encoder_layer_3_intermediate_dense_Add_output_0 = encoder_layer_3_intermediate_dense_bias + _encoder_layer_3_intermediate_dense_MatMul_output_0;
    auto _encoder_layer_3_intermediate_intermediate_act_fn_Div_output_0 = _encoder_layer_3_intermediate_dense_Add_output_0 / _encoder_layer_0_intermediate_intermediate_act_fn_Constant_output_0;
    auto _encoder_layer_3_intermediate_intermediate_act_fn_Erf_output_0 = erf<true>(_encoder_layer_3_intermediate_intermediate_act_fn_Div_output_0);
    auto _encoder_layer_3_intermediate_intermediate_act_fn_Add_output_0 = _encoder_layer_3_intermediate_intermediate_act_fn_Erf_output_0 + _Constant_18_output_0;
    auto _encoder_layer_3_intermediate_intermediate_act_fn_Mul_output_0 = _encoder_layer_3_intermediate_dense_Add_output_0 * _encoder_layer_3_intermediate_intermediate_act_fn_Add_output_0;
    auto _encoder_layer_3_intermediate_intermediate_act_fn_Mul_1_output_0 = _encoder_layer_3_intermediate_intermediate_act_fn_Mul_output_0 * _encoder_layer_0_intermediate_intermediate_act_fn_Constant_2_output_0;
    auto _encoder_layer_3_output_dense_MatMul_output_0 = matmul<1>(_encoder_layer_3_intermediate_intermediate_act_fn_Mul_1_output_0, _encoder_layer_3_output_dense_Transpose_output_0);
    auto _encoder_layer_3_output_dense_Add_output_0 = encoder_layer_3_output_dense_bias + _encoder_layer_3_output_dense_MatMul_output_0;
    auto _encoder_layer_3_output_Add_output_0 = _encoder_layer_3_output_dense_Add_output_0 + _encoder_layer_3_attention_output_LayerNorm_LayerNormalization_output_0;
    auto _encoder_layer_3_output_LayerNorm_LayerNormalization_output_0 = layer_norm<-1>(_encoder_layer_3_output_Add_output_0, encoder_layer_3_output_LayerNorm_weight, encoder_layer_3_output_LayerNorm_bias, 9.999999960041972e-13);
    auto _encoder_layer_4_attention_self_query_MatMul_output_0 = matmul<1>(_encoder_layer_3_output_LayerNorm_LayerNormalization_output_0, _encoder_layer_4_attention_self_query_Transpose_output_0);
    auto _encoder_layer_4_attention_self_query_Add_output_0 = encoder_layer_4_attention_self_query_bias + _encoder_layer_4_attention_self_query_MatMul_output_0;
    auto _encoder_layer_4_attention_self_Reshape_output_0 = _encoder_layer_4_attention_self_query_Add_output_0.reshape<1,512,12,64>();
    auto _encoder_layer_4_attention_self_Transpose_output_0 = _encoder_layer_4_attention_self_Reshape_output_0.transpose<0,2,1,3>();
    auto _encoder_layer_4_attention_self_key_MatMul_output_0 = matmul<1>(_encoder_layer_3_output_LayerNorm_LayerNormalization_output_0, _encoder_layer_4_attention_self_key_Transpose_output_0);
    auto _encoder_layer_4_attention_self_key_Add_output_0 = encoder_layer_4_attention_self_key_bias + _encoder_layer_4_attention_self_key_MatMul_output_0;
    auto _encoder_layer_4_attention_self_Reshape_1_output_0 = _encoder_layer_4_attention_self_key_Add_output_0.reshape<1,512,12,64>();
    auto _encoder_layer_4_attention_self_value_MatMul_output_0 = matmul<1>(_encoder_layer_3_output_LayerNorm_LayerNormalization_output_0, _encoder_layer_4_attention_self_value_Transpose_output_0);
    auto _encoder_layer_4_attention_self_value_Add_output_0 = encoder_layer_4_attention_self_value_bias + _encoder_layer_4_attention_self_value_MatMul_output_0;
    auto _encoder_layer_4_attention_self_Reshape_2_output_0 = _encoder_layer_4_attention_self_value_Add_output_0.reshape<1,512,12,64>();
    auto _encoder_layer_4_attention_self_Transpose_1_output_0 = _encoder_layer_4_attention_self_Reshape_2_output_0.transpose<0,2,1,3>();
    auto _encoder_layer_4_attention_self_Transpose_2_output_0 = _encoder_layer_4_attention_self_Reshape_1_output_0.transpose<0,2,3,1>();
    auto _encoder_layer_4_attention_self_Mul_output_0 = _encoder_layer_4_attention_self_Transpose_output_0 * _encoder_layer_0_attention_self_Sqrt_1_output_0;
    auto _encoder_layer_4_attention_self_Mul_1_output_0 = _encoder_layer_4_attention_self_Transpose_2_output_0 * _encoder_layer_0_attention_self_Sqrt_1_output_0;
    auto _encoder_layer_4_attention_self_MatMul_output_0 = matmul<1>(_encoder_layer_4_attention_self_Mul_output_0, _encoder_layer_4_attention_self_Mul_1_output_0);
    auto _encoder_layer_4_attention_self_Softmax_output_0 = softmax<-1, true>(_encoder_layer_4_attention_self_MatMul_output_0);
    auto _encoder_layer_4_attention_self_MatMul_1_output_0 = matmul<1>(_encoder_layer_4_attention_self_Softmax_output_0, _encoder_layer_4_attention_self_Transpose_1_output_0);
    auto _encoder_layer_4_attention_self_Transpose_3_output_0 = _encoder_layer_4_attention_self_MatMul_1_output_0.transpose<0,2,1,3>();
    auto _encoder_layer_4_attention_self_Reshape_3_output_0 = _encoder_layer_4_attention_self_Transpose_3_output_0.reshape<1,512,768>();
    auto _encoder_layer_4_attention_output_dense_MatMul_output_0 = matmul<1>(_encoder_layer_4_attention_self_Reshape_3_output_0, _encoder_layer_4_attention_output_dense_Transpose_output_0);
    auto _encoder_layer_4_attention_output_dense_Add_output_0 = encoder_layer_4_attention_output_dense_bias + _encoder_layer_4_attention_output_dense_MatMul_output_0;
    auto _encoder_layer_4_attention_output_Add_output_0 = _encoder_layer_4_attention_output_dense_Add_output_0 + _encoder_layer_3_output_LayerNorm_LayerNormalization_output_0;
    auto _encoder_layer_4_attention_output_LayerNorm_LayerNormalization_output_0 = layer_norm<-1>(_encoder_layer_4_attention_output_Add_output_0, encoder_layer_4_attention_output_LayerNorm_weight, encoder_layer_4_attention_output_LayerNorm_bias, 9.999999960041972e-13);
    auto _encoder_layer_4_intermediate_dense_MatMul_output_0 = matmul<1>(_encoder_layer_4_attention_output_LayerNorm_LayerNormalization_output_0, _encoder_layer_4_intermediate_dense_Transpose_output_0);
    auto _encoder_layer_4_intermediate_dense_Add_output_0 = encoder_layer_4_intermediate_dense_bias + _encoder_layer_4_intermediate_dense_MatMul_output_0;
    auto _encoder_layer_4_intermediate_intermediate_act_fn_Div_output_0 = _encoder_layer_4_intermediate_dense_Add_output_0 / _encoder_layer_0_intermediate_intermediate_act_fn_Constant_output_0;
    auto _encoder_layer_4_intermediate_intermediate_act_fn_Erf_output_0 = erf<true>(_encoder_layer_4_intermediate_intermediate_act_fn_Div_output_0);
    auto _encoder_layer_4_intermediate_intermediate_act_fn_Add_output_0 = _encoder_layer_4_intermediate_intermediate_act_fn_Erf_output_0 + _Constant_18_output_0;
    auto _encoder_layer_4_intermediate_intermediate_act_fn_Mul_output_0 = _encoder_layer_4_intermediate_dense_Add_output_0 * _encoder_layer_4_intermediate_intermediate_act_fn_Add_output_0;
    auto _encoder_layer_4_intermediate_intermediate_act_fn_Mul_1_output_0 = _encoder_layer_4_intermediate_intermediate_act_fn_Mul_output_0 * _encoder_layer_0_intermediate_intermediate_act_fn_Constant_2_output_0;
    auto _encoder_layer_4_output_dense_MatMul_output_0 = matmul<1>(_encoder_layer_4_intermediate_intermediate_act_fn_Mul_1_output_0, _encoder_layer_4_output_dense_Transpose_output_0);
    auto _encoder_layer_4_output_dense_Add_output_0 = encoder_layer_4_output_dense_bias + _encoder_layer_4_output_dense_MatMul_output_0;
    auto _encoder_layer_4_output_Add_output_0 = _encoder_layer_4_output_dense_Add_output_0 + _encoder_layer_4_attention_output_LayerNorm_LayerNormalization_output_0;
    auto _encoder_layer_4_output_LayerNorm_LayerNormalization_output_0 = layer_norm<-1>(_encoder_layer_4_output_Add_output_0, encoder_layer_4_output_LayerNorm_weight, encoder_layer_4_output_LayerNorm_bias, 9.999999960041972e-13);
    auto _encoder_layer_5_attention_self_query_MatMul_output_0 = matmul<1>(_encoder_layer_4_output_LayerNorm_LayerNormalization_output_0, _encoder_layer_5_attention_self_query_Transpose_output_0);
    auto _encoder_layer_5_attention_self_query_Add_output_0 = encoder_layer_5_attention_self_query_bias + _encoder_layer_5_attention_self_query_MatMul_output_0;
    auto _encoder_layer_5_attention_self_Reshape_output_0 = _encoder_layer_5_attention_self_query_Add_output_0.reshape<1,512,12,64>();
    auto _encoder_layer_5_attention_self_Transpose_output_0 = _encoder_layer_5_attention_self_Reshape_output_0.transpose<0,2,1,3>();
    auto _encoder_layer_5_attention_self_key_MatMul_output_0 = matmul<1>(_encoder_layer_4_output_LayerNorm_LayerNormalization_output_0, _encoder_layer_5_attention_self_key_Transpose_output_0);
    auto _encoder_layer_5_attention_self_key_Add_output_0 = encoder_layer_5_attention_self_key_bias + _encoder_layer_5_attention_self_key_MatMul_output_0;
    auto _encoder_layer_5_attention_self_Reshape_1_output_0 = _encoder_layer_5_attention_self_key_Add_output_0.reshape<1,512,12,64>();
    auto _encoder_layer_5_attention_self_value_MatMul_output_0 = matmul<1>(_encoder_layer_4_output_LayerNorm_LayerNormalization_output_0, _encoder_layer_5_attention_self_value_Transpose_output_0);
    auto _encoder_layer_5_attention_self_value_Add_output_0 = encoder_layer_5_attention_self_value_bias + _encoder_layer_5_attention_self_value_MatMul_output_0;
    auto _encoder_layer_5_attention_self_Reshape_2_output_0 = _encoder_layer_5_attention_self_value_Add_output_0.reshape<1,512,12,64>();
    auto _encoder_layer_5_attention_self_Transpose_1_output_0 = _encoder_layer_5_attention_self_Reshape_2_output_0.transpose<0,2,1,3>();
    auto _encoder_layer_5_attention_self_Transpose_2_output_0 = _encoder_layer_5_attention_self_Reshape_1_output_0.transpose<0,2,3,1>();
    auto _encoder_layer_5_attention_self_Mul_output_0 = _encoder_layer_5_attention_self_Transpose_output_0 * _encoder_layer_0_attention_self_Sqrt_1_output_0;
    auto _encoder_layer_5_attention_self_Mul_1_output_0 = _encoder_layer_5_attention_self_Transpose_2_output_0 * _encoder_layer_0_attention_self_Sqrt_1_output_0;
    auto _encoder_layer_5_attention_self_MatMul_output_0 = matmul<1>(_encoder_layer_5_attention_self_Mul_output_0, _encoder_layer_5_attention_self_Mul_1_output_0);
    auto _encoder_layer_5_attention_self_Softmax_output_0 = softmax<-1, true>(_encoder_layer_5_attention_self_MatMul_output_0);
    auto _encoder_layer_5_attention_self_MatMul_1_output_0 = matmul<1>(_encoder_layer_5_attention_self_Softmax_output_0, _encoder_layer_5_attention_self_Transpose_1_output_0);
    auto _encoder_layer_5_attention_self_Transpose_3_output_0 = _encoder_layer_5_attention_self_MatMul_1_output_0.transpose<0,2,1,3>();
    auto _encoder_layer_5_attention_self_Reshape_3_output_0 = _encoder_layer_5_attention_self_Transpose_3_output_0.reshape<1,512,768>();
    auto _encoder_layer_5_attention_output_dense_MatMul_output_0 = matmul<1>(_encoder_layer_5_attention_self_Reshape_3_output_0, _encoder_layer_5_attention_output_dense_Transpose_output_0);
    auto _encoder_layer_5_attention_output_dense_Add_output_0 = encoder_layer_5_attention_output_dense_bias + _encoder_layer_5_attention_output_dense_MatMul_output_0;
    auto _encoder_layer_5_attention_output_Add_output_0 = _encoder_layer_5_attention_output_dense_Add_output_0 + _encoder_layer_4_output_LayerNorm_LayerNormalization_output_0;
    auto _encoder_layer_5_attention_output_LayerNorm_LayerNormalization_output_0 = layer_norm<-1>(_encoder_layer_5_attention_output_Add_output_0, encoder_layer_5_attention_output_LayerNorm_weight, encoder_layer_5_attention_output_LayerNorm_bias, 9.999999960041972e-13);
    auto _encoder_layer_5_intermediate_dense_MatMul_output_0 = matmul<1>(_encoder_layer_5_attention_output_LayerNorm_LayerNormalization_output_0, _encoder_layer_5_intermediate_dense_Transpose_output_0);
    auto _encoder_layer_5_intermediate_dense_Add_output_0 = encoder_layer_5_intermediate_dense_bias + _encoder_layer_5_intermediate_dense_MatMul_output_0;
    auto _encoder_layer_5_intermediate_intermediate_act_fn_Div_output_0 = _encoder_layer_5_intermediate_dense_Add_output_0 / _encoder_layer_0_intermediate_intermediate_act_fn_Constant_output_0;
    auto _encoder_layer_5_intermediate_intermediate_act_fn_Erf_output_0 = erf<true>(_encoder_layer_5_intermediate_intermediate_act_fn_Div_output_0);
    auto _encoder_layer_5_intermediate_intermediate_act_fn_Add_output_0 = _encoder_layer_5_intermediate_intermediate_act_fn_Erf_output_0 + _Constant_18_output_0;
    auto _encoder_layer_5_intermediate_intermediate_act_fn_Mul_output_0 = _encoder_layer_5_intermediate_dense_Add_output_0 * _encoder_layer_5_intermediate_intermediate_act_fn_Add_output_0;
    auto _encoder_layer_5_intermediate_intermediate_act_fn_Mul_1_output_0 = _encoder_layer_5_intermediate_intermediate_act_fn_Mul_output_0 * _encoder_layer_0_intermediate_intermediate_act_fn_Constant_2_output_0;
    auto _encoder_layer_5_output_dense_MatMul_output_0 = matmul<1>(_encoder_layer_5_intermediate_intermediate_act_fn_Mul_1_output_0, _encoder_layer_5_output_dense_Transpose_output_0);
    auto _encoder_layer_5_output_dense_Add_output_0 = encoder_layer_5_output_dense_bias + _encoder_layer_5_output_dense_MatMul_output_0;
    auto _encoder_layer_5_output_Add_output_0 = _encoder_layer_5_output_dense_Add_output_0 + _encoder_layer_5_attention_output_LayerNorm_LayerNormalization_output_0;
    auto _encoder_layer_5_output_LayerNorm_LayerNormalization_output_0 = layer_norm<-1>(_encoder_layer_5_output_Add_output_0, encoder_layer_5_output_LayerNorm_weight, encoder_layer_5_output_LayerNorm_bias, 9.999999960041972e-13);
    auto _encoder_layer_6_attention_self_query_MatMul_output_0 = matmul<1>(_encoder_layer_5_output_LayerNorm_LayerNormalization_output_0, _encoder_layer_6_attention_self_query_Transpose_output_0);
    auto _encoder_layer_6_attention_self_query_Add_output_0 = encoder_layer_6_attention_self_query_bias + _encoder_layer_6_attention_self_query_MatMul_output_0;
    auto _encoder_layer_6_attention_self_Reshape_output_0 = _encoder_layer_6_attention_self_query_Add_output_0.reshape<1,512,12,64>();
    auto _encoder_layer_6_attention_self_Transpose_output_0 = _encoder_layer_6_attention_self_Reshape_output_0.transpose<0,2,1,3>();
    auto _encoder_layer_6_attention_self_key_MatMul_output_0 = matmul<1>(_encoder_layer_5_output_LayerNorm_LayerNormalization_output_0, _encoder_layer_6_attention_self_key_Transpose_output_0);
    auto _encoder_layer_6_attention_self_key_Add_output_0 = encoder_layer_6_attention_self_key_bias + _encoder_layer_6_attention_self_key_MatMul_output_0;
    auto _encoder_layer_6_attention_self_Reshape_1_output_0 = _encoder_layer_6_attention_self_key_Add_output_0.reshape<1,512,12,64>();
    auto _encoder_layer_6_attention_self_value_MatMul_output_0 = matmul<1>(_encoder_layer_5_output_LayerNorm_LayerNormalization_output_0, _encoder_layer_6_attention_self_value_Transpose_output_0);
    auto _encoder_layer_6_attention_self_value_Add_output_0 = encoder_layer_6_attention_self_value_bias + _encoder_layer_6_attention_self_value_MatMul_output_0;
    auto _encoder_layer_6_attention_self_Reshape_2_output_0 = _encoder_layer_6_attention_self_value_Add_output_0.reshape<1,512,12,64>();
    auto _encoder_layer_6_attention_self_Transpose_1_output_0 = _encoder_layer_6_attention_self_Reshape_2_output_0.transpose<0,2,1,3>();
    auto _encoder_layer_6_attention_self_Transpose_2_output_0 = _encoder_layer_6_attention_self_Reshape_1_output_0.transpose<0,2,3,1>();
    auto _encoder_layer_6_attention_self_Mul_output_0 = _encoder_layer_6_attention_self_Transpose_output_0 * _encoder_layer_0_attention_self_Sqrt_1_output_0;
    auto _encoder_layer_6_attention_self_Mul_1_output_0 = _encoder_layer_6_attention_self_Transpose_2_output_0 * _encoder_layer_0_attention_self_Sqrt_1_output_0;
    auto _encoder_layer_6_attention_self_MatMul_output_0 = matmul<1>(_encoder_layer_6_attention_self_Mul_output_0, _encoder_layer_6_attention_self_Mul_1_output_0);
    auto _encoder_layer_6_attention_self_Softmax_output_0 = softmax<-1, true>(_encoder_layer_6_attention_self_MatMul_output_0);
    auto _encoder_layer_6_attention_self_MatMul_1_output_0 = matmul<1>(_encoder_layer_6_attention_self_Softmax_output_0, _encoder_layer_6_attention_self_Transpose_1_output_0);
    auto _encoder_layer_6_attention_self_Transpose_3_output_0 = _encoder_layer_6_attention_self_MatMul_1_output_0.transpose<0,2,1,3>();
    auto _encoder_layer_6_attention_self_Reshape_3_output_0 = _encoder_layer_6_attention_self_Transpose_3_output_0.reshape<1,512,768>();
    auto _encoder_layer_6_attention_output_dense_MatMul_output_0 = matmul<1>(_encoder_layer_6_attention_self_Reshape_3_output_0, _encoder_layer_6_attention_output_dense_Transpose_output_0);
    auto _encoder_layer_6_attention_output_dense_Add_output_0 = encoder_layer_6_attention_output_dense_bias + _encoder_layer_6_attention_output_dense_MatMul_output_0;
    auto _encoder_layer_6_attention_output_Add_output_0 = _encoder_layer_6_attention_output_dense_Add_output_0 + _encoder_layer_5_output_LayerNorm_LayerNormalization_output_0;
    auto _encoder_layer_6_attention_output_LayerNorm_LayerNormalization_output_0 = layer_norm<-1>(_encoder_layer_6_attention_output_Add_output_0, encoder_layer_6_attention_output_LayerNorm_weight, encoder_layer_6_attention_output_LayerNorm_bias, 9.999999960041972e-13);
    auto _encoder_layer_6_intermediate_dense_MatMul_output_0 = matmul<1>(_encoder_layer_6_attention_output_LayerNorm_LayerNormalization_output_0, _encoder_layer_6_intermediate_dense_Transpose_output_0);
    auto _encoder_layer_6_intermediate_dense_Add_output_0 = encoder_layer_6_intermediate_dense_bias + _encoder_layer_6_intermediate_dense_MatMul_output_0;
    auto _encoder_layer_6_intermediate_intermediate_act_fn_Div_output_0 = _encoder_layer_6_intermediate_dense_Add_output_0 / _encoder_layer_0_intermediate_intermediate_act_fn_Constant_output_0;
    auto _encoder_layer_6_intermediate_intermediate_act_fn_Erf_output_0 = erf<true>(_encoder_layer_6_intermediate_intermediate_act_fn_Div_output_0);
    auto _encoder_layer_6_intermediate_intermediate_act_fn_Add_output_0 = _encoder_layer_6_intermediate_intermediate_act_fn_Erf_output_0 + _Constant_18_output_0;
    auto _encoder_layer_6_intermediate_intermediate_act_fn_Mul_output_0 = _encoder_layer_6_intermediate_dense_Add_output_0 * _encoder_layer_6_intermediate_intermediate_act_fn_Add_output_0;
    auto _encoder_layer_6_intermediate_intermediate_act_fn_Mul_1_output_0 = _encoder_layer_6_intermediate_intermediate_act_fn_Mul_output_0 * _encoder_layer_0_intermediate_intermediate_act_fn_Constant_2_output_0;
    auto _encoder_layer_6_output_dense_MatMul_output_0 = matmul<1>(_encoder_layer_6_intermediate_intermediate_act_fn_Mul_1_output_0, _encoder_layer_6_output_dense_Transpose_output_0);
    auto _encoder_layer_6_output_dense_Add_output_0 = encoder_layer_6_output_dense_bias + _encoder_layer_6_output_dense_MatMul_output_0;
    auto _encoder_layer_6_output_Add_output_0 = _encoder_layer_6_output_dense_Add_output_0 + _encoder_layer_6_attention_output_LayerNorm_LayerNormalization_output_0;
    auto _encoder_layer_6_output_LayerNorm_LayerNormalization_output_0 = layer_norm<-1>(_encoder_layer_6_output_Add_output_0, encoder_layer_6_output_LayerNorm_weight, encoder_layer_6_output_LayerNorm_bias, 9.999999960041972e-13);
    auto _encoder_layer_7_attention_self_query_MatMul_output_0 = matmul<1>(_encoder_layer_6_output_LayerNorm_LayerNormalization_output_0, _encoder_layer_7_attention_self_query_Transpose_output_0);
    auto _encoder_layer_7_attention_self_query_Add_output_0 = encoder_layer_7_attention_self_query_bias + _encoder_layer_7_attention_self_query_MatMul_output_0;
    auto _encoder_layer_7_attention_self_Reshape_output_0 = _encoder_layer_7_attention_self_query_Add_output_0.reshape<1,512,12,64>();
    auto _encoder_layer_7_attention_self_Transpose_output_0 = _encoder_layer_7_attention_self_Reshape_output_0.transpose<0,2,1,3>();
    auto _encoder_layer_7_attention_self_key_MatMul_output_0 = matmul<1>(_encoder_layer_6_output_LayerNorm_LayerNormalization_output_0, _encoder_layer_7_attention_self_key_Transpose_output_0);
    auto _encoder_layer_7_attention_self_key_Add_output_0 = encoder_layer_7_attention_self_key_bias + _encoder_layer_7_attention_self_key_MatMul_output_0;
    auto _encoder_layer_7_attention_self_Reshape_1_output_0 = _encoder_layer_7_attention_self_key_Add_output_0.reshape<1,512,12,64>();
    auto _encoder_layer_7_attention_self_value_MatMul_output_0 = matmul<1>(_encoder_layer_6_output_LayerNorm_LayerNormalization_output_0, _encoder_layer_7_attention_self_value_Transpose_output_0);
    auto _encoder_layer_7_attention_self_value_Add_output_0 = encoder_layer_7_attention_self_value_bias + _encoder_layer_7_attention_self_value_MatMul_output_0;
    auto _encoder_layer_7_attention_self_Reshape_2_output_0 = _encoder_layer_7_attention_self_value_Add_output_0.reshape<1,512,12,64>();
    auto _encoder_layer_7_attention_self_Transpose_1_output_0 = _encoder_layer_7_attention_self_Reshape_2_output_0.transpose<0,2,1,3>();
    auto _encoder_layer_7_attention_self_Transpose_2_output_0 = _encoder_layer_7_attention_self_Reshape_1_output_0.transpose<0,2,3,1>();
    auto _encoder_layer_7_attention_self_Mul_output_0 = _encoder_layer_7_attention_self_Transpose_output_0 * _encoder_layer_0_attention_self_Sqrt_1_output_0;
    auto _encoder_layer_7_attention_self_Mul_1_output_0 = _encoder_layer_7_attention_self_Transpose_2_output_0 * _encoder_layer_0_attention_self_Sqrt_1_output_0;
    auto _encoder_layer_7_attention_self_MatMul_output_0 = matmul<1>(_encoder_layer_7_attention_self_Mul_output_0, _encoder_layer_7_attention_self_Mul_1_output_0);
    auto _encoder_layer_7_attention_self_Softmax_output_0 = softmax<-1, true>(_encoder_layer_7_attention_self_MatMul_output_0);
    auto _encoder_layer_7_attention_self_MatMul_1_output_0 = matmul<1>(_encoder_layer_7_attention_self_Softmax_output_0, _encoder_layer_7_attention_self_Transpose_1_output_0);
    auto _encoder_layer_7_attention_self_Transpose_3_output_0 = _encoder_layer_7_attention_self_MatMul_1_output_0.transpose<0,2,1,3>();
    auto _encoder_layer_7_attention_self_Reshape_3_output_0 = _encoder_layer_7_attention_self_Transpose_3_output_0.reshape<1,512,768>();
    auto _encoder_layer_7_attention_output_dense_MatMul_output_0 = matmul<1>(_encoder_layer_7_attention_self_Reshape_3_output_0, _encoder_layer_7_attention_output_dense_Transpose_output_0);
    auto _encoder_layer_7_attention_output_dense_Add_output_0 = encoder_layer_7_attention_output_dense_bias + _encoder_layer_7_attention_output_dense_MatMul_output_0;
    auto _encoder_layer_7_attention_output_Add_output_0 = _encoder_layer_7_attention_output_dense_Add_output_0 + _encoder_layer_6_output_LayerNorm_LayerNormalization_output_0;
    auto _encoder_layer_7_attention_output_LayerNorm_LayerNormalization_output_0 = layer_norm<-1>(_encoder_layer_7_attention_output_Add_output_0, encoder_layer_7_attention_output_LayerNorm_weight, encoder_layer_7_attention_output_LayerNorm_bias, 9.999999960041972e-13);
    auto _encoder_layer_7_intermediate_dense_MatMul_output_0 = matmul<1>(_encoder_layer_7_attention_output_LayerNorm_LayerNormalization_output_0, _encoder_layer_7_intermediate_dense_Transpose_output_0);
    auto _encoder_layer_7_intermediate_dense_Add_output_0 = encoder_layer_7_intermediate_dense_bias + _encoder_layer_7_intermediate_dense_MatMul_output_0;
    auto _encoder_layer_7_intermediate_intermediate_act_fn_Div_output_0 = _encoder_layer_7_intermediate_dense_Add_output_0 / _encoder_layer_0_intermediate_intermediate_act_fn_Constant_output_0;
    auto _encoder_layer_7_intermediate_intermediate_act_fn_Erf_output_0 = erf<true>(_encoder_layer_7_intermediate_intermediate_act_fn_Div_output_0);
    auto _encoder_layer_7_intermediate_intermediate_act_fn_Add_output_0 = _encoder_layer_7_intermediate_intermediate_act_fn_Erf_output_0 + _Constant_18_output_0;
    auto _encoder_layer_7_intermediate_intermediate_act_fn_Mul_output_0 = _encoder_layer_7_intermediate_dense_Add_output_0 * _encoder_layer_7_intermediate_intermediate_act_fn_Add_output_0;
    auto _encoder_layer_7_intermediate_intermediate_act_fn_Mul_1_output_0 = _encoder_layer_7_intermediate_intermediate_act_fn_Mul_output_0 * _encoder_layer_0_intermediate_intermediate_act_fn_Constant_2_output_0;
    auto _encoder_layer_7_output_dense_MatMul_output_0 = matmul<1>(_encoder_layer_7_intermediate_intermediate_act_fn_Mul_1_output_0, _encoder_layer_7_output_dense_Transpose_output_0);
    auto _encoder_layer_7_output_dense_Add_output_0 = encoder_layer_7_output_dense_bias + _encoder_layer_7_output_dense_MatMul_output_0;
    auto _encoder_layer_7_output_Add_output_0 = _encoder_layer_7_output_dense_Add_output_0 + _encoder_layer_7_attention_output_LayerNorm_LayerNormalization_output_0;
    auto _encoder_layer_7_output_LayerNorm_LayerNormalization_output_0 = layer_norm<-1>(_encoder_layer_7_output_Add_output_0, encoder_layer_7_output_LayerNorm_weight, encoder_layer_7_output_LayerNorm_bias, 9.999999960041972e-13);
    auto _encoder_layer_8_attention_self_query_MatMul_output_0 = matmul<1>(_encoder_layer_7_output_LayerNorm_LayerNormalization_output_0, _encoder_layer_8_attention_self_query_Transpose_output_0);
    auto _encoder_layer_8_attention_self_query_Add_output_0 = encoder_layer_8_attention_self_query_bias + _encoder_layer_8_attention_self_query_MatMul_output_0;
    auto _encoder_layer_8_attention_self_Reshape_output_0 = _encoder_layer_8_attention_self_query_Add_output_0.reshape<1,512,12,64>();
    auto _encoder_layer_8_attention_self_Transpose_output_0 = _encoder_layer_8_attention_self_Reshape_output_0.transpose<0,2,1,3>();
    auto _encoder_layer_8_attention_self_key_MatMul_output_0 = matmul<1>(_encoder_layer_7_output_LayerNorm_LayerNormalization_output_0, _encoder_layer_8_attention_self_key_Transpose_output_0);
    auto _encoder_layer_8_attention_self_key_Add_output_0 = encoder_layer_8_attention_self_key_bias + _encoder_layer_8_attention_self_key_MatMul_output_0;
    auto _encoder_layer_8_attention_self_Reshape_1_output_0 = _encoder_layer_8_attention_self_key_Add_output_0.reshape<1,512,12,64>();
    auto _encoder_layer_8_attention_self_value_MatMul_output_0 = matmul<1>(_encoder_layer_7_output_LayerNorm_LayerNormalization_output_0, _encoder_layer_8_attention_self_value_Transpose_output_0);
    auto _encoder_layer_8_attention_self_value_Add_output_0 = encoder_layer_8_attention_self_value_bias + _encoder_layer_8_attention_self_value_MatMul_output_0;
    auto _encoder_layer_8_attention_self_Reshape_2_output_0 = _encoder_layer_8_attention_self_value_Add_output_0.reshape<1,512,12,64>();
    auto _encoder_layer_8_attention_self_Transpose_1_output_0 = _encoder_layer_8_attention_self_Reshape_2_output_0.transpose<0,2,1,3>();
    auto _encoder_layer_8_attention_self_Transpose_2_output_0 = _encoder_layer_8_attention_self_Reshape_1_output_0.transpose<0,2,3,1>();
    auto _encoder_layer_8_attention_self_Mul_output_0 = _encoder_layer_8_attention_self_Transpose_output_0 * _encoder_layer_0_attention_self_Sqrt_1_output_0;
    auto _encoder_layer_8_attention_self_Mul_1_output_0 = _encoder_layer_8_attention_self_Transpose_2_output_0 * _encoder_layer_0_attention_self_Sqrt_1_output_0;
    auto _encoder_layer_8_attention_self_MatMul_output_0 = matmul<1>(_encoder_layer_8_attention_self_Mul_output_0, _encoder_layer_8_attention_self_Mul_1_output_0);
    auto _encoder_layer_8_attention_self_Softmax_output_0 = softmax<-1, true>(_encoder_layer_8_attention_self_MatMul_output_0);
    auto _encoder_layer_8_attention_self_MatMul_1_output_0 = matmul<1>(_encoder_layer_8_attention_self_Softmax_output_0, _encoder_layer_8_attention_self_Transpose_1_output_0);
    auto _encoder_layer_8_attention_self_Transpose_3_output_0 = _encoder_layer_8_attention_self_MatMul_1_output_0.transpose<0,2,1,3>();
    auto _encoder_layer_8_attention_self_Reshape_3_output_0 = _encoder_layer_8_attention_self_Transpose_3_output_0.reshape<1,512,768>();
    auto _encoder_layer_8_attention_output_dense_MatMul_output_0 = matmul<1>(_encoder_layer_8_attention_self_Reshape_3_output_0, _encoder_layer_8_attention_output_dense_Transpose_output_0);
    auto _encoder_layer_8_attention_output_dense_Add_output_0 = encoder_layer_8_attention_output_dense_bias + _encoder_layer_8_attention_output_dense_MatMul_output_0;
    auto _encoder_layer_8_attention_output_Add_output_0 = _encoder_layer_8_attention_output_dense_Add_output_0 + _encoder_layer_7_output_LayerNorm_LayerNormalization_output_0;
    auto _encoder_layer_8_attention_output_LayerNorm_LayerNormalization_output_0 = layer_norm<-1>(_encoder_layer_8_attention_output_Add_output_0, encoder_layer_8_attention_output_LayerNorm_weight, encoder_layer_8_attention_output_LayerNorm_bias, 9.999999960041972e-13);
    auto _encoder_layer_8_intermediate_dense_MatMul_output_0 = matmul<1>(_encoder_layer_8_attention_output_LayerNorm_LayerNormalization_output_0, _encoder_layer_8_intermediate_dense_Transpose_output_0);
    auto _encoder_layer_8_intermediate_dense_Add_output_0 = encoder_layer_8_intermediate_dense_bias + _encoder_layer_8_intermediate_dense_MatMul_output_0;
    auto _encoder_layer_8_intermediate_intermediate_act_fn_Div_output_0 = _encoder_layer_8_intermediate_dense_Add_output_0 / _encoder_layer_0_intermediate_intermediate_act_fn_Constant_output_0;
    auto _encoder_layer_8_intermediate_intermediate_act_fn_Erf_output_0 = erf<true>(_encoder_layer_8_intermediate_intermediate_act_fn_Div_output_0);
    auto _encoder_layer_8_intermediate_intermediate_act_fn_Add_output_0 = _encoder_layer_8_intermediate_intermediate_act_fn_Erf_output_0 + _Constant_18_output_0;
    auto _encoder_layer_8_intermediate_intermediate_act_fn_Mul_output_0 = _encoder_layer_8_intermediate_dense_Add_output_0 * _encoder_layer_8_intermediate_intermediate_act_fn_Add_output_0;
    auto _encoder_layer_8_intermediate_intermediate_act_fn_Mul_1_output_0 = _encoder_layer_8_intermediate_intermediate_act_fn_Mul_output_0 * _encoder_layer_0_intermediate_intermediate_act_fn_Constant_2_output_0;
    auto _encoder_layer_8_output_dense_MatMul_output_0 = matmul<1>(_encoder_layer_8_intermediate_intermediate_act_fn_Mul_1_output_0, _encoder_layer_8_output_dense_Transpose_output_0);
    auto _encoder_layer_8_output_dense_Add_output_0 = encoder_layer_8_output_dense_bias + _encoder_layer_8_output_dense_MatMul_output_0;
    auto _encoder_layer_8_output_Add_output_0 = _encoder_layer_8_output_dense_Add_output_0 + _encoder_layer_8_attention_output_LayerNorm_LayerNormalization_output_0;
    auto _encoder_layer_8_output_LayerNorm_LayerNormalization_output_0 = layer_norm<-1>(_encoder_layer_8_output_Add_output_0, encoder_layer_8_output_LayerNorm_weight, encoder_layer_8_output_LayerNorm_bias, 9.999999960041972e-13);
    auto _encoder_layer_9_attention_self_query_MatMul_output_0 = matmul<1>(_encoder_layer_8_output_LayerNorm_LayerNormalization_output_0, _encoder_layer_9_attention_self_query_Transpose_output_0);
    auto _encoder_layer_9_attention_self_query_Add_output_0 = encoder_layer_9_attention_self_query_bias + _encoder_layer_9_attention_self_query_MatMul_output_0;
    auto _encoder_layer_9_attention_self_Reshape_output_0 = _encoder_layer_9_attention_self_query_Add_output_0.reshape<1,512,12,64>();
    auto _encoder_layer_9_attention_self_Transpose_output_0 = _encoder_layer_9_attention_self_Reshape_output_0.transpose<0,2,1,3>();
    auto _encoder_layer_9_attention_self_key_MatMul_output_0 = matmul<1>(_encoder_layer_8_output_LayerNorm_LayerNormalization_output_0, _encoder_layer_9_attention_self_key_Transpose_output_0);
    auto _encoder_layer_9_attention_self_key_Add_output_0 = encoder_layer_9_attention_self_key_bias + _encoder_layer_9_attention_self_key_MatMul_output_0;
    auto _encoder_layer_9_attention_self_Reshape_1_output_0 = _encoder_layer_9_attention_self_key_Add_output_0.reshape<1,512,12,64>();
    auto _encoder_layer_9_attention_self_value_MatMul_output_0 = matmul<1>(_encoder_layer_8_output_LayerNorm_LayerNormalization_output_0, _encoder_layer_9_attention_self_value_Transpose_output_0);
    auto _encoder_layer_9_attention_self_value_Add_output_0 = encoder_layer_9_attention_self_value_bias + _encoder_layer_9_attention_self_value_MatMul_output_0;
    auto _encoder_layer_9_attention_self_Reshape_2_output_0 = _encoder_layer_9_attention_self_value_Add_output_0.reshape<1,512,12,64>();
    auto _encoder_layer_9_attention_self_Transpose_1_output_0 = _encoder_layer_9_attention_self_Reshape_2_output_0.transpose<0,2,1,3>();
    auto _encoder_layer_9_attention_self_Transpose_2_output_0 = _encoder_layer_9_attention_self_Reshape_1_output_0.transpose<0,2,3,1>();
    auto _encoder_layer_9_attention_self_Mul_output_0 = _encoder_layer_9_attention_self_Transpose_output_0 * _encoder_layer_0_attention_self_Sqrt_1_output_0;
    auto _encoder_layer_9_attention_self_Mul_1_output_0 = _encoder_layer_9_attention_self_Transpose_2_output_0 * _encoder_layer_0_attention_self_Sqrt_1_output_0;
    auto _encoder_layer_9_attention_self_MatMul_output_0 = matmul<1>(_encoder_layer_9_attention_self_Mul_output_0, _encoder_layer_9_attention_self_Mul_1_output_0);
    auto _encoder_layer_9_attention_self_Softmax_output_0 = softmax<-1, true>(_encoder_layer_9_attention_self_MatMul_output_0);
    auto _encoder_layer_9_attention_self_MatMul_1_output_0 = matmul<1>(_encoder_layer_9_attention_self_Softmax_output_0, _encoder_layer_9_attention_self_Transpose_1_output_0);
    auto _encoder_layer_9_attention_self_Transpose_3_output_0 = _encoder_layer_9_attention_self_MatMul_1_output_0.transpose<0,2,1,3>();
    auto _encoder_layer_9_attention_self_Reshape_3_output_0 = _encoder_layer_9_attention_self_Transpose_3_output_0.reshape<1,512,768>();
    auto _encoder_layer_9_attention_output_dense_MatMul_output_0 = matmul<1>(_encoder_layer_9_attention_self_Reshape_3_output_0, _encoder_layer_9_attention_output_dense_Transpose_output_0);
    auto _encoder_layer_9_attention_output_dense_Add_output_0 = encoder_layer_9_attention_output_dense_bias + _encoder_layer_9_attention_output_dense_MatMul_output_0;
    auto _encoder_layer_9_attention_output_Add_output_0 = _encoder_layer_9_attention_output_dense_Add_output_0 + _encoder_layer_8_output_LayerNorm_LayerNormalization_output_0;
    auto _encoder_layer_9_attention_output_LayerNorm_LayerNormalization_output_0 = layer_norm<-1>(_encoder_layer_9_attention_output_Add_output_0, encoder_layer_9_attention_output_LayerNorm_weight, encoder_layer_9_attention_output_LayerNorm_bias, 9.999999960041972e-13);
    auto _encoder_layer_9_intermediate_dense_MatMul_output_0 = matmul<1>(_encoder_layer_9_attention_output_LayerNorm_LayerNormalization_output_0, _encoder_layer_9_intermediate_dense_Transpose_output_0);
    auto _encoder_layer_9_intermediate_dense_Add_output_0 = encoder_layer_9_intermediate_dense_bias + _encoder_layer_9_intermediate_dense_MatMul_output_0;
    auto _encoder_layer_9_intermediate_intermediate_act_fn_Div_output_0 = _encoder_layer_9_intermediate_dense_Add_output_0 / _encoder_layer_0_intermediate_intermediate_act_fn_Constant_output_0;
    auto _encoder_layer_9_intermediate_intermediate_act_fn_Erf_output_0 = erf<true>(_encoder_layer_9_intermediate_intermediate_act_fn_Div_output_0);
    auto _encoder_layer_9_intermediate_intermediate_act_fn_Add_output_0 = _encoder_layer_9_intermediate_intermediate_act_fn_Erf_output_0 + _Constant_18_output_0;
    auto _encoder_layer_9_intermediate_intermediate_act_fn_Mul_output_0 = _encoder_layer_9_intermediate_dense_Add_output_0 * _encoder_layer_9_intermediate_intermediate_act_fn_Add_output_0;
    auto _encoder_layer_9_intermediate_intermediate_act_fn_Mul_1_output_0 = _encoder_layer_9_intermediate_intermediate_act_fn_Mul_output_0 * _encoder_layer_0_intermediate_intermediate_act_fn_Constant_2_output_0;
    auto _encoder_layer_9_output_dense_MatMul_output_0 = matmul<1>(_encoder_layer_9_intermediate_intermediate_act_fn_Mul_1_output_0, _encoder_layer_9_output_dense_Transpose_output_0);
    auto _encoder_layer_9_output_dense_Add_output_0 = encoder_layer_9_output_dense_bias + _encoder_layer_9_output_dense_MatMul_output_0;
    auto _encoder_layer_9_output_Add_output_0 = _encoder_layer_9_output_dense_Add_output_0 + _encoder_layer_9_attention_output_LayerNorm_LayerNormalization_output_0;
    auto _encoder_layer_9_output_LayerNorm_LayerNormalization_output_0 = layer_norm<-1>(_encoder_layer_9_output_Add_output_0, encoder_layer_9_output_LayerNorm_weight, encoder_layer_9_output_LayerNorm_bias, 9.999999960041972e-13);
    auto _encoder_layer_10_attention_self_query_MatMul_output_0 = matmul<1>(_encoder_layer_9_output_LayerNorm_LayerNormalization_output_0, _encoder_layer_10_attention_self_query_Transpose_output_0);
    auto _encoder_layer_10_attention_self_query_Add_output_0 = encoder_layer_10_attention_self_query_bias + _encoder_layer_10_attention_self_query_MatMul_output_0;
    auto _encoder_layer_10_attention_self_Reshape_output_0 = _encoder_layer_10_attention_self_query_Add_output_0.reshape<1,512,12,64>();
    auto _encoder_layer_10_attention_self_Transpose_output_0 = _encoder_layer_10_attention_self_Reshape_output_0.transpose<0,2,1,3>();
    auto _encoder_layer_10_attention_self_key_MatMul_output_0 = matmul<1>(_encoder_layer_9_output_LayerNorm_LayerNormalization_output_0, _encoder_layer_10_attention_self_key_Transpose_output_0);
    auto _encoder_layer_10_attention_self_key_Add_output_0 = encoder_layer_10_attention_self_key_bias + _encoder_layer_10_attention_self_key_MatMul_output_0;
    auto _encoder_layer_10_attention_self_Reshape_1_output_0 = _encoder_layer_10_attention_self_key_Add_output_0.reshape<1,512,12,64>();
    auto _encoder_layer_10_attention_self_value_MatMul_output_0 = matmul<1>(_encoder_layer_9_output_LayerNorm_LayerNormalization_output_0, _encoder_layer_10_attention_self_value_Transpose_output_0);
    auto _encoder_layer_10_attention_self_value_Add_output_0 = encoder_layer_10_attention_self_value_bias + _encoder_layer_10_attention_self_value_MatMul_output_0;
    auto _encoder_layer_10_attention_self_Reshape_2_output_0 = _encoder_layer_10_attention_self_value_Add_output_0.reshape<1,512,12,64>();
    auto _encoder_layer_10_attention_self_Transpose_1_output_0 = _encoder_layer_10_attention_self_Reshape_2_output_0.transpose<0,2,1,3>();
    auto _encoder_layer_10_attention_self_Transpose_2_output_0 = _encoder_layer_10_attention_self_Reshape_1_output_0.transpose<0,2,3,1>();
    auto _encoder_layer_10_attention_self_Mul_output_0 = _encoder_layer_10_attention_self_Transpose_output_0 * _encoder_layer_0_attention_self_Sqrt_1_output_0;
    auto _encoder_layer_10_attention_self_Mul_1_output_0 = _encoder_layer_10_attention_self_Transpose_2_output_0 * _encoder_layer_0_attention_self_Sqrt_1_output_0;
    auto _encoder_layer_10_attention_self_MatMul_output_0 = matmul<1>(_encoder_layer_10_attention_self_Mul_output_0, _encoder_layer_10_attention_self_Mul_1_output_0);
    auto _encoder_layer_10_attention_self_Softmax_output_0 = softmax<-1, true>(_encoder_layer_10_attention_self_MatMul_output_0);
    auto _encoder_layer_10_attention_self_MatMul_1_output_0 = matmul<1>(_encoder_layer_10_attention_self_Softmax_output_0, _encoder_layer_10_attention_self_Transpose_1_output_0);
    auto _encoder_layer_10_attention_self_Transpose_3_output_0 = _encoder_layer_10_attention_self_MatMul_1_output_0.transpose<0,2,1,3>();
    auto _encoder_layer_10_attention_self_Reshape_3_output_0 = _encoder_layer_10_attention_self_Transpose_3_output_0.reshape<1,512,768>();
    auto _encoder_layer_10_attention_output_dense_MatMul_output_0 = matmul<1>(_encoder_layer_10_attention_self_Reshape_3_output_0, _encoder_layer_10_attention_output_dense_Transpose_output_0);
    auto _encoder_layer_10_attention_output_dense_Add_output_0 = encoder_layer_10_attention_output_dense_bias + _encoder_layer_10_attention_output_dense_MatMul_output_0;
    auto _encoder_layer_10_attention_output_Add_output_0 = _encoder_layer_10_attention_output_dense_Add_output_0 + _encoder_layer_9_output_LayerNorm_LayerNormalization_output_0;
    auto _encoder_layer_10_attention_output_LayerNorm_LayerNormalization_output_0 = layer_norm<-1>(_encoder_layer_10_attention_output_Add_output_0, encoder_layer_10_attention_output_LayerNorm_weight, encoder_layer_10_attention_output_LayerNorm_bias, 9.999999960041972e-13);
    auto _encoder_layer_10_intermediate_dense_MatMul_output_0 = matmul<1>(_encoder_layer_10_attention_output_LayerNorm_LayerNormalization_output_0, _encoder_layer_10_intermediate_dense_Transpose_output_0);
    auto _encoder_layer_10_intermediate_dense_Add_output_0 = encoder_layer_10_intermediate_dense_bias + _encoder_layer_10_intermediate_dense_MatMul_output_0;
    auto _encoder_layer_10_intermediate_intermediate_act_fn_Div_output_0 = _encoder_layer_10_intermediate_dense_Add_output_0 / _encoder_layer_0_intermediate_intermediate_act_fn_Constant_output_0;
    auto _encoder_layer_10_intermediate_intermediate_act_fn_Erf_output_0 = erf<true>(_encoder_layer_10_intermediate_intermediate_act_fn_Div_output_0);
    auto _encoder_layer_10_intermediate_intermediate_act_fn_Add_output_0 = _encoder_layer_10_intermediate_intermediate_act_fn_Erf_output_0 + _Constant_18_output_0;
    auto _encoder_layer_10_intermediate_intermediate_act_fn_Mul_output_0 = _encoder_layer_10_intermediate_dense_Add_output_0 * _encoder_layer_10_intermediate_intermediate_act_fn_Add_output_0;
    auto _encoder_layer_10_intermediate_intermediate_act_fn_Mul_1_output_0 = _encoder_layer_10_intermediate_intermediate_act_fn_Mul_output_0 * _encoder_layer_0_intermediate_intermediate_act_fn_Constant_2_output_0;
    auto _encoder_layer_10_output_dense_MatMul_output_0 = matmul<1>(_encoder_layer_10_intermediate_intermediate_act_fn_Mul_1_output_0, _encoder_layer_10_output_dense_Transpose_output_0);
    auto _encoder_layer_10_output_dense_Add_output_0 = encoder_layer_10_output_dense_bias + _encoder_layer_10_output_dense_MatMul_output_0;
    auto _encoder_layer_10_output_Add_output_0 = _encoder_layer_10_output_dense_Add_output_0 + _encoder_layer_10_attention_output_LayerNorm_LayerNormalization_output_0;
    auto _encoder_layer_10_output_LayerNorm_LayerNormalization_output_0 = layer_norm<-1>(_encoder_layer_10_output_Add_output_0, encoder_layer_10_output_LayerNorm_weight, encoder_layer_10_output_LayerNorm_bias, 9.999999960041972e-13);
    auto _encoder_layer_11_attention_self_query_MatMul_output_0 = matmul<1>(_encoder_layer_10_output_LayerNorm_LayerNormalization_output_0, _encoder_layer_11_attention_self_query_Transpose_output_0);
    auto _encoder_layer_11_attention_self_query_Add_output_0 = encoder_layer_11_attention_self_query_bias + _encoder_layer_11_attention_self_query_MatMul_output_0;
    auto _encoder_layer_11_attention_self_Reshape_output_0 = _encoder_layer_11_attention_self_query_Add_output_0.reshape<1,512,12,64>();
    auto _encoder_layer_11_attention_self_Transpose_output_0 = _encoder_layer_11_attention_self_Reshape_output_0.transpose<0,2,1,3>();
    auto _encoder_layer_11_attention_self_key_MatMul_output_0 = matmul<1>(_encoder_layer_10_output_LayerNorm_LayerNormalization_output_0, _encoder_layer_11_attention_self_key_Transpose_output_0);
    auto _encoder_layer_11_attention_self_key_Add_output_0 = encoder_layer_11_attention_self_key_bias + _encoder_layer_11_attention_self_key_MatMul_output_0;
    auto _encoder_layer_11_attention_self_Reshape_1_output_0 = _encoder_layer_11_attention_self_key_Add_output_0.reshape<1,512,12,64>();
    auto _encoder_layer_11_attention_self_value_MatMul_output_0 = matmul<1>(_encoder_layer_10_output_LayerNorm_LayerNormalization_output_0, _encoder_layer_11_attention_self_value_Transpose_output_0);
    auto _encoder_layer_11_attention_self_value_Add_output_0 = encoder_layer_11_attention_self_value_bias + _encoder_layer_11_attention_self_value_MatMul_output_0;
    auto _encoder_layer_11_attention_self_Reshape_2_output_0 = _encoder_layer_11_attention_self_value_Add_output_0.reshape<1,512,12,64>();
    auto _encoder_layer_11_attention_self_Transpose_1_output_0 = _encoder_layer_11_attention_self_Reshape_2_output_0.transpose<0,2,1,3>();
    auto _encoder_layer_11_attention_self_Transpose_2_output_0 = _encoder_layer_11_attention_self_Reshape_1_output_0.transpose<0,2,3,1>();
    auto _encoder_layer_11_attention_self_Mul_output_0 = _encoder_layer_11_attention_self_Transpose_output_0 * _encoder_layer_0_attention_self_Sqrt_1_output_0;
    auto _encoder_layer_11_attention_self_Mul_1_output_0 = _encoder_layer_11_attention_self_Transpose_2_output_0 * _encoder_layer_0_attention_self_Sqrt_1_output_0;
    auto _encoder_layer_11_attention_self_MatMul_output_0 = matmul<1>(_encoder_layer_11_attention_self_Mul_output_0, _encoder_layer_11_attention_self_Mul_1_output_0);
    auto _encoder_layer_11_attention_self_Softmax_output_0 = softmax<-1, true>(_encoder_layer_11_attention_self_MatMul_output_0);
    auto _encoder_layer_11_attention_self_MatMul_1_output_0 = matmul<1>(_encoder_layer_11_attention_self_Softmax_output_0, _encoder_layer_11_attention_self_Transpose_1_output_0);
    auto _encoder_layer_11_attention_self_Transpose_3_output_0 = _encoder_layer_11_attention_self_MatMul_1_output_0.transpose<0,2,1,3>();
    auto _encoder_layer_11_attention_self_Reshape_3_output_0 = _encoder_layer_11_attention_self_Transpose_3_output_0.reshape<1,512,768>();
    auto _encoder_layer_11_attention_output_dense_MatMul_output_0 = matmul<1>(_encoder_layer_11_attention_self_Reshape_3_output_0, _encoder_layer_11_attention_output_dense_Transpose_output_0);
    auto _encoder_layer_11_attention_output_dense_Add_output_0 = encoder_layer_11_attention_output_dense_bias + _encoder_layer_11_attention_output_dense_MatMul_output_0;
    auto _encoder_layer_11_attention_output_Add_output_0 = _encoder_layer_11_attention_output_dense_Add_output_0 + _encoder_layer_10_output_LayerNorm_LayerNormalization_output_0;
    auto _encoder_layer_11_attention_output_LayerNorm_LayerNormalization_output_0 = layer_norm<-1>(_encoder_layer_11_attention_output_Add_output_0, encoder_layer_11_attention_output_LayerNorm_weight, encoder_layer_11_attention_output_LayerNorm_bias, 9.999999960041972e-13);
    auto _encoder_layer_11_intermediate_dense_MatMul_output_0 = matmul<1>(_encoder_layer_11_attention_output_LayerNorm_LayerNormalization_output_0, _encoder_layer_11_intermediate_dense_Transpose_output_0);
    auto _encoder_layer_11_intermediate_dense_Add_output_0 = encoder_layer_11_intermediate_dense_bias + _encoder_layer_11_intermediate_dense_MatMul_output_0;
    auto _encoder_layer_11_intermediate_intermediate_act_fn_Div_output_0 = _encoder_layer_11_intermediate_dense_Add_output_0 / _encoder_layer_0_intermediate_intermediate_act_fn_Constant_output_0;
    auto _encoder_layer_11_intermediate_intermediate_act_fn_Erf_output_0 = erf<true>(_encoder_layer_11_intermediate_intermediate_act_fn_Div_output_0);
    auto _encoder_layer_11_intermediate_intermediate_act_fn_Add_output_0 = _encoder_layer_11_intermediate_intermediate_act_fn_Erf_output_0 + _Constant_18_output_0;
    auto _encoder_layer_11_intermediate_intermediate_act_fn_Mul_output_0 = _encoder_layer_11_intermediate_dense_Add_output_0 * _encoder_layer_11_intermediate_intermediate_act_fn_Add_output_0;
    auto _encoder_layer_11_intermediate_intermediate_act_fn_Mul_1_output_0 = _encoder_layer_11_intermediate_intermediate_act_fn_Mul_output_0 * _encoder_layer_0_intermediate_intermediate_act_fn_Constant_2_output_0;
    auto _encoder_layer_11_output_dense_MatMul_output_0 = matmul<1>(_encoder_layer_11_intermediate_intermediate_act_fn_Mul_1_output_0, _encoder_layer_11_output_dense_Transpose_output_0);
    auto _encoder_layer_11_output_dense_Add_output_0 = encoder_layer_11_output_dense_bias + _encoder_layer_11_output_dense_MatMul_output_0;
    auto _encoder_layer_11_output_Add_output_0 = _encoder_layer_11_output_dense_Add_output_0 + _encoder_layer_11_attention_output_LayerNorm_LayerNormalization_output_0;
    auto onnx_Gather_1420 = layer_norm<-1>(_encoder_layer_11_output_Add_output_0, encoder_layer_11_output_LayerNorm_weight, encoder_layer_11_output_LayerNorm_bias, 9.999999960041972e-13);
    auto _pooler_Gather_output_0 = onnx_Gather_1420(0);
    auto _pooler_dense_Gemm_output_0 = matmul<1.0, 1.0>(_pooler_Gather_output_0, pooler_dense_weight, pooler_dense_bias);
    auto _1423 = tanh<true>(_pooler_dense_Gemm_output_0);
    return std::make_tuple(onnx_Gather_1420, _1423);

}
};


int main() {
    InitGuard guard;
    Model mod("../test.bin");
    Tensor<int32_t, ALLOCATOR, Sequence<1,512>> input_1;

    while(true)
    mod.forward(input_1);
};